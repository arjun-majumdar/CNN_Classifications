{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZOD2mGIeyEJ"
   },
   "source": [
    "# ResNet-50 CNN: PyTorch & CIFAR-10\n",
    "\n",
    "End-to-end programming tutorial including:\n",
    "\n",
    "1. Progress bar - training model\n",
    "1. Train model with _early stopping criterion_\n",
    "1. Learning rate scheduler\n",
    "1. Compare between learning rate scheduler and early stopping criterion\n",
    "\n",
    "\n",
    "#### References \n",
    "- [Dive into Deep Learning - ResNet reference](https://d2l.ai/chapter_convolutional-modern/resnet.html)\n",
    "\n",
    "- [YouTube reference](https://www.youtube.com/watch?v=DkNIBBBvcPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to ResNet research paper: __Deep Residual Learning for Image Recognition__ by Kaiming He et al.\n",
    "\n",
    "In page 4, Figure 3. ResNet-34 architecture is shown. It can learn more complex and new features in these 2 layers. But it's also going to use the skip connection/identity mapping from it previously leanred/coputed. So the CNN can kind of choose what it wants to learn. Either a combination of what it has learned before (skip connection) and the new things it has learned using the 2 conv layers within a ResNet block. The argument here is that the CNN is going to learn new things but it's at least never going to forget what it learned before. So, in theory, it should never become worse as we increase the depth of the CNN. Hence, by increasing the depth of the CNN, it never worsens the performance.\n",
    "\n",
    "In page 5, Table 1, different architecture specific details are specified. In this tutorial, ResNet-50/101 and 152 will be implemented. The first conv layer: kernel size = (7, 7), stride = 2, number of kernels = 64, padding = 3; Max pool: kernel size = (3, 3), stride =2 \n",
    "\n",
    "ResNet-50 has Four ResNet layers. If we look at the first ResNet layer, it has a block here which is-\n",
    "- 1x1 filter with 64 channels\n",
    "- 3x3, 64\n",
    "- 1x1, 256\n",
    "\n",
    "it repeats/performs this block 3 times.\n",
    "\n",
    "Same convolutions - none of them change the size of input. Stride is used to reduce the spatial output in each of the conv layers.\n",
    "\n",
    "One more thing to note is that if we look at the input channels for the first ResNet layer/block, it is 64 and the number of channels at the end is 256. For ResNet layer/block 2, the number of input channels is 128 and the number of channels at the end is 512. For ResNet layer/block 3, the number of input channels is 256 and the number of channels at the end is 1024. The ResNet architecture follows the pattern that the output channel is going to be 4x the input channel for that particular ResNet layer/block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7hpnDY7Mfb8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "# Specify GPU to be used-\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cM6Ih3nVq0Gh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3DvYMsqNezdm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ha-v5IiWezqk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_H9KbTonq0Q9",
    "outputId": "41b72a44-64e1-44c2-a917-615e9adafb76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently available device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration-\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"currently available device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs = 1\n"
     ]
    }
   ],
   "source": [
    "# Get number of GPUs-\n",
    "print(f\"Number of available GPUs = {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU = 0\n"
     ]
    }
   ],
   "source": [
    "# Check the current GPU-\n",
    "print(f\"Current GPU = {torch.cuda.current_device()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of current GPU = Quadro M6000\n"
     ]
    }
   ],
   "source": [
    "# Get the name of the current GPU\n",
    "print(f\"Name of current GPU = {torch.cuda.get_device_name(torch.cuda.current_device())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch using a GPU? True\n"
     ]
    }
   ],
   "source": [
    "# Is PyTorch using a GPU?\n",
    "print(f\"PyTorch using a GPU? {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f2CJ2XyNq9Nd",
    "outputId": "3e5961db-ccb3-4be4-9207-21d280f44beb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.8.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k6rTKbcZrBN0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uV1m3vD0rBee"
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters-\n",
    "num_epochs = 65\n",
    "batch_size = 128\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of epochs for training = 65 with default LR = 0.01\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of epochs for training = {num_epochs} with default LR = {learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6QGc3oYHrDzL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9R6Sc5v6rD8M"
   },
   "outputs": [],
   "source": [
    "# Define transformations for training and test sets-\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "      transforms.RandomCrop(32, padding = 4),\n",
    "      transforms.RandomHorizontalFlip(),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "     ]\n",
    "     )\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "     ]\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "id": "4ddOxdkirHPN",
    "outputId": "764e40cc-2f3c-482f-bbb1-542c39d0d1f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load dataset-\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root = './data', train = True,\n",
    "        download = True, transform = transform_train\n",
    "        )\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root = './data', train = False,\n",
    "        download = True, transform = transform_test\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FW8h3ztJrU5_",
    "outputId": "643be603-0775-412b-9a3d-7dfbe5df1887"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_dataset) = 50000 & len(test_dataset) = 10000\n"
     ]
    }
   ],
   "source": [
    "print(f\"len(train_dataset) = {len(train_dataset)} & len(test_dataset) = {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "cydrwaW7rbnV"
   },
   "outputs": [],
   "source": [
    "# Create training and testing loaders-\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size = batch_size,\n",
    "        shuffle = True\n",
    "        )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size = batch_size,\n",
    "        shuffle = False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62AgWl9jrfzW",
    "outputId": "0972fead-2556-4cea-9ff4-8f50eabf6cef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_loader) = 391 & len(test_loader) = 79\n"
     ]
    }
   ],
   "source": [
    "print(f\"len(train_loader) = {len(train_loader)} & len(test_loader) = {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rjGADCMfrhAl",
    "outputId": "e3bb3177-720d-48ce-ef5a-b0e61c5ac9bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(390.625, 78.125)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check-\n",
    "len(train_dataset) / batch_size, len(test_dataset) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEBunYxErhh8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 3, 32, 32]), torch.Size([128]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check-\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "images.size(), labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jhwdHTu0rsf8",
    "outputId": "c84eaa78-265f-4af7-dbdf-f45370c2bb4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 3, 32, 32]), torch.Size([128]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check-\n",
    "len(train_dataset) / batch_size, len(test_dataset) / batch_size\n",
    "\n",
    "# Sanity check-\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "images.size(), labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlFoyg5Hruqn"
   },
   "source": [
    "### Define _ResNet-50_ architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic _ResNet_ block\n",
    "\n",
    "```identity_downsample``` is a conv layer which we might need to use depending on if we have changed the input size or if we have changed the number of channels. Hence, we need to adapt the identity so that we can use it later on when we have used a few conv layers. We use ```identity_downsample``` layer if we need to change the shape in some way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_block(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, identity_downsample = None, stride = 1):\n",
    "        super(ResNet_block, self).__init__()\n",
    "\n",
    "        # number of channels after a block is 4x of what it entered/was passed-\n",
    "        self.expansion = 4\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels = input_channels, out_channels = output_channels,\n",
    "            kernel_size = 1, stride = 1,\n",
    "            padding = 0, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features = output_channels)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels = output_channels, out_channels = output_channels,\n",
    "            kernel_size = 3, stride = stride,\n",
    "            padding = 1, bias = False)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features = output_channels)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels = output_channels, out_channels = output_channels * self.expansion,\n",
    "            kernel_size = 1, stride = 1,\n",
    "            padding = 0, bias = False)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features = output_channels * self.expansion)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # A conv layer-\n",
    "        self.identity_downsample = identity_downsample\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        \n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "        \n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            # print(m)\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform_(m.weight)\n",
    "\n",
    "                '''\n",
    "                # Do not initialize bias (due to batchnorm)-\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "                '''\n",
    "            \n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                # Standard initialization for batch normalization-\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet-50/101/152 architecture is _slightly_ changed for CIFAR-10 dataset instead of ImageNet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    '''\n",
    "    layers - a Python3 list specifying the number of times to use 'ResNet_block'\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, ResNet_block, layers, image_channels = 3, num_classes = 10):\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        self.input_channels = 64\n",
    "        \n",
    "        '''\n",
    "        # For ImageNet-\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels = image_channels, out_channels = 64,\n",
    "            kernel_size = 7, stride = 2,\n",
    "            padding = 3, bias = False)\n",
    "        '''\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels = image_channels, out_channels = 64,\n",
    "            kernel_size = 3, stride = 1,\n",
    "            padding = 1, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features = 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        '''\n",
    "        # For ImageNet-\n",
    "        self.maxpool = nn.MaxPool2d(\n",
    "            kernel_size = 3, stride = 2,\n",
    "            padding = 1\n",
    "            )\n",
    "        '''\n",
    "        \n",
    "        # ResNet blocks-\n",
    "        self.layer1 = self._make_layer(ResNet_block, layers[0], output_channels = 64, stride = 1)\n",
    "        self.layer2 = self._make_layer(ResNet_block, layers[1], output_channels = 128, stride = 2)\n",
    "        self.layer3 = self._make_layer(ResNet_block, layers[2], output_channels = 256, stride = 2)\n",
    "        self.layer4 = self._make_layer(ResNet_block, layers[3], output_channels = 512, stride = 2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * 4, num_classes)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        # x = self.maxpool(x)  # For ImageNet\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        # Reshape before passing to dense layer-\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "    def _make_layer(self, ResNet_block, num_residual_blocks, output_channels, stride):\n",
    "        identity_downsample = None\n",
    "        layers = []\n",
    "        \n",
    "        '''\n",
    "        We want to know when are we going to actually use/do an identity_downsample? When are we going to\n",
    "        have the conv layer change the identity?\n",
    "        1. Either we change the input size\n",
    "        2.\n",
    "        '''\n",
    "        if stride != 1 or self.input_channels != output_channels * 4:\n",
    "            identity_downsample = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels = self.input_channels, out_channels = 4 * output_channels,\n",
    "                    kernel_size = 1, stride = stride,\n",
    "                    bias = False),\n",
    "                nn.BatchNorm2d(num_features = output_channels * 4)\n",
    "                )\n",
    "        \n",
    "        # This is the layer that changes the number of channels-\n",
    "        layers.append(ResNet_block(self.input_channels, output_channels, identity_downsample, stride))\n",
    "        # After this first block, the number of channels is going to be changed\n",
    "        \n",
    "        self.input_channels = output_channels * 4       # 64 x 4 = 256\n",
    "        # At the end of the first block, the output = 256\n",
    "        \n",
    "        for i in range(num_residual_blocks - 1):\n",
    "            layers.append(ResNet_block(self.input_channels, output_channels))\n",
    "        \n",
    "        \n",
    "        return (nn.Sequential(*layers))\n",
    "        # *layers unpacks the list so that PyTorch knows that each comes after the other\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50(img_channels, num_channels = 1000):\n",
    "    # Function to define ResNet-50 architecture\n",
    "    return ResNet(ResNet_block, [3, 4, 6, 3], img_channels, num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet101(img_channels, num_channels = 1000):\n",
    "    # Function to define ResNet-101 architecture\n",
    "    return ResNet(ResNet_block, [3, 4, 23, 3], img_channels, num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet152(img_channels, num_channels = 1000):\n",
    "    # Function to define ResNet-152 architecture\n",
    "    return ResNet(ResNet_block, [3, 8, 36, 3], img_channels, num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    # Three images of (32, 32, 3). Number of in_channels = 3-\n",
    "    x = torch.randn(3, 3, 32, 32)\n",
    "    \n",
    "    y = model(x).to(device)\n",
    "    print(f\"Output.shape: {y.shape}\")\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a ResNet-50 model-\n",
    "model = ResNet50(img_channels = 3, num_channels = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 10])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check-\n",
    "model(images).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output.shape: torch.Size([3, 10])\n"
     ]
    }
   ],
   "source": [
    "# Sanity check-\n",
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (layer1): Sequential(\n",
      "    (0): ResNet_block(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResNet_block(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): ResNet_block(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): ResNet_block(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResNet_block(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): ResNet_block(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (3): ResNet_block(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): ResNet_block(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResNet_block(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): ResNet_block(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (3): ResNet_block(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (4): ResNet_block(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (5): ResNet_block(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): ResNet_block(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (identity_downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResNet_block(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): ResNet_block(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q9aSBydxsD27",
    "outputId": "79ba1ecd-97d6-4c18-fea7-2acb2cbe66f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (layer1): Sequential(\n",
       "    (0): ResNet_block(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (identity_downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResNet_block(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): ResNet_block(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): ResNet_block(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (identity_downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResNet_block(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): ResNet_block(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (3): ResNet_block(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): ResNet_block(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (identity_downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResNet_block(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): ResNet_block(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (3): ResNet_block(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (4): ResNet_block(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (5): ResNet_block(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): ResNet_block(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (identity_downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResNet_block(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): ResNet_block(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Place model on GPU-\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1SnL7fjksJLh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9A-AfIXwcE6u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q8zux0jjsHq0",
    "outputId": "e986b7ad-171b-474c-cb60-2097fdecc112"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer.shape = torch.Size([64, 3, 3, 3]) has 1728 parameters\n",
      "layer.shape = torch.Size([64]) has 64 parameters\n",
      "layer.shape = torch.Size([64]) has 64 parameters\n",
      "layer.shape = torch.Size([64, 64, 1, 1]) has 4096 parameters\n",
      "layer.shape = torch.Size([64]) has 64 parameters\n",
      "layer.shape = torch.Size([64]) has 64 parameters\n",
      "layer.shape = torch.Size([64, 64, 3, 3]) has 36864 parameters\n",
      "layer.shape = torch.Size([64]) has 64 parameters\n",
      "layer.shape = torch.Size([64]) has 64 parameters\n",
      "layer.shape = torch.Size([256, 64, 1, 1]) has 16384 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([256, 64, 1, 1]) has 16384 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([64, 256, 1, 1]) has 16384 parameters\n",
      "layer.shape = torch.Size([64]) has 64 parameters\n",
      "layer.shape = torch.Size([64]) has 64 parameters\n",
      "layer.shape = torch.Size([64, 64, 3, 3]) has 36864 parameters\n",
      "layer.shape = torch.Size([64]) has 64 parameters\n",
      "layer.shape = torch.Size([64]) has 64 parameters\n",
      "layer.shape = torch.Size([256, 64, 1, 1]) has 16384 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([64, 256, 1, 1]) has 16384 parameters\n",
      "layer.shape = torch.Size([64]) has 64 parameters\n",
      "layer.shape = torch.Size([64]) has 64 parameters\n",
      "layer.shape = torch.Size([64, 64, 3, 3]) has 36864 parameters\n",
      "layer.shape = torch.Size([64]) has 64 parameters\n",
      "layer.shape = torch.Size([64]) has 64 parameters\n",
      "layer.shape = torch.Size([256, 64, 1, 1]) has 16384 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([128, 256, 1, 1]) has 32768 parameters\n",
      "layer.shape = torch.Size([128]) has 128 parameters\n",
      "layer.shape = torch.Size([128]) has 128 parameters\n",
      "layer.shape = torch.Size([128, 128, 3, 3]) has 147456 parameters\n",
      "layer.shape = torch.Size([128]) has 128 parameters\n",
      "layer.shape = torch.Size([128]) has 128 parameters\n",
      "layer.shape = torch.Size([512, 128, 1, 1]) has 65536 parameters\n",
      "layer.shape = torch.Size([512]) has 512 parameters\n",
      "layer.shape = torch.Size([512]) has 512 parameters\n",
      "layer.shape = torch.Size([512, 256, 1, 1]) has 131072 parameters\n",
      "layer.shape = torch.Size([512]) has 512 parameters\n",
      "layer.shape = torch.Size([512]) has 512 parameters\n",
      "layer.shape = torch.Size([128, 512, 1, 1]) has 65536 parameters\n",
      "layer.shape = torch.Size([128]) has 128 parameters\n",
      "layer.shape = torch.Size([128]) has 128 parameters\n",
      "layer.shape = torch.Size([128, 128, 3, 3]) has 147456 parameters\n",
      "layer.shape = torch.Size([128]) has 128 parameters\n",
      "layer.shape = torch.Size([128]) has 128 parameters\n",
      "layer.shape = torch.Size([512, 128, 1, 1]) has 65536 parameters\n",
      "layer.shape = torch.Size([512]) has 512 parameters\n",
      "layer.shape = torch.Size([512]) has 512 parameters\n",
      "layer.shape = torch.Size([128, 512, 1, 1]) has 65536 parameters\n",
      "layer.shape = torch.Size([128]) has 128 parameters\n",
      "layer.shape = torch.Size([128]) has 128 parameters\n",
      "layer.shape = torch.Size([128, 128, 3, 3]) has 147456 parameters\n",
      "layer.shape = torch.Size([128]) has 128 parameters\n",
      "layer.shape = torch.Size([128]) has 128 parameters\n",
      "layer.shape = torch.Size([512, 128, 1, 1]) has 65536 parameters\n",
      "layer.shape = torch.Size([512]) has 512 parameters\n",
      "layer.shape = torch.Size([512]) has 512 parameters\n",
      "layer.shape = torch.Size([128, 512, 1, 1]) has 65536 parameters\n",
      "layer.shape = torch.Size([128]) has 128 parameters\n",
      "layer.shape = torch.Size([128]) has 128 parameters\n",
      "layer.shape = torch.Size([128, 128, 3, 3]) has 147456 parameters\n",
      "layer.shape = torch.Size([128]) has 128 parameters\n",
      "layer.shape = torch.Size([128]) has 128 parameters\n",
      "layer.shape = torch.Size([512, 128, 1, 1]) has 65536 parameters\n",
      "layer.shape = torch.Size([512]) has 512 parameters\n",
      "layer.shape = torch.Size([512]) has 512 parameters\n",
      "layer.shape = torch.Size([256, 512, 1, 1]) has 131072 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([256, 256, 3, 3]) has 589824 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([1024, 256, 1, 1]) has 262144 parameters\n",
      "layer.shape = torch.Size([1024]) has 1024 parameters\n",
      "layer.shape = torch.Size([1024]) has 1024 parameters\n",
      "layer.shape = torch.Size([1024, 512, 1, 1]) has 524288 parameters\n",
      "layer.shape = torch.Size([1024]) has 1024 parameters\n",
      "layer.shape = torch.Size([1024]) has 1024 parameters\n",
      "layer.shape = torch.Size([256, 1024, 1, 1]) has 262144 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([256, 256, 3, 3]) has 589824 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([1024, 256, 1, 1]) has 262144 parameters\n",
      "layer.shape = torch.Size([1024]) has 1024 parameters\n",
      "layer.shape = torch.Size([1024]) has 1024 parameters\n",
      "layer.shape = torch.Size([256, 1024, 1, 1]) has 262144 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([256, 256, 3, 3]) has 589824 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([1024, 256, 1, 1]) has 262144 parameters\n",
      "layer.shape = torch.Size([1024]) has 1024 parameters\n",
      "layer.shape = torch.Size([1024]) has 1024 parameters\n",
      "layer.shape = torch.Size([256, 1024, 1, 1]) has 262144 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([256, 256, 3, 3]) has 589824 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([1024, 256, 1, 1]) has 262144 parameters\n",
      "layer.shape = torch.Size([1024]) has 1024 parameters\n",
      "layer.shape = torch.Size([1024]) has 1024 parameters\n",
      "layer.shape = torch.Size([256, 1024, 1, 1]) has 262144 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([256, 256, 3, 3]) has 589824 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([1024, 256, 1, 1]) has 262144 parameters\n",
      "layer.shape = torch.Size([1024]) has 1024 parameters\n",
      "layer.shape = torch.Size([1024]) has 1024 parameters\n",
      "layer.shape = torch.Size([256, 1024, 1, 1]) has 262144 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([256, 256, 3, 3]) has 589824 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([256]) has 256 parameters\n",
      "layer.shape = torch.Size([1024, 256, 1, 1]) has 262144 parameters\n",
      "layer.shape = torch.Size([1024]) has 1024 parameters\n",
      "layer.shape = torch.Size([1024]) has 1024 parameters\n",
      "layer.shape = torch.Size([512, 1024, 1, 1]) has 524288 parameters\n",
      "layer.shape = torch.Size([512]) has 512 parameters\n",
      "layer.shape = torch.Size([512]) has 512 parameters\n",
      "layer.shape = torch.Size([512, 512, 3, 3]) has 2359296 parameters\n",
      "layer.shape = torch.Size([512]) has 512 parameters\n",
      "layer.shape = torch.Size([512]) has 512 parameters\n",
      "layer.shape = torch.Size([2048, 512, 1, 1]) has 1048576 parameters\n",
      "layer.shape = torch.Size([2048]) has 2048 parameters\n",
      "layer.shape = torch.Size([2048]) has 2048 parameters\n",
      "layer.shape = torch.Size([2048, 1024, 1, 1]) has 2097152 parameters\n",
      "layer.shape = torch.Size([2048]) has 2048 parameters\n",
      "layer.shape = torch.Size([2048]) has 2048 parameters\n",
      "layer.shape = torch.Size([512, 2048, 1, 1]) has 1048576 parameters\n",
      "layer.shape = torch.Size([512]) has 512 parameters\n",
      "layer.shape = torch.Size([512]) has 512 parameters\n",
      "layer.shape = torch.Size([512, 512, 3, 3]) has 2359296 parameters\n",
      "layer.shape = torch.Size([512]) has 512 parameters\n",
      "layer.shape = torch.Size([512]) has 512 parameters\n",
      "layer.shape = torch.Size([2048, 512, 1, 1]) has 1048576 parameters\n",
      "layer.shape = torch.Size([2048]) has 2048 parameters\n",
      "layer.shape = torch.Size([2048]) has 2048 parameters\n",
      "layer.shape = torch.Size([512, 2048, 1, 1]) has 1048576 parameters\n",
      "layer.shape = torch.Size([512]) has 512 parameters\n",
      "layer.shape = torch.Size([512]) has 512 parameters\n",
      "layer.shape = torch.Size([512, 512, 3, 3]) has 2359296 parameters\n",
      "layer.shape = torch.Size([512]) has 512 parameters\n",
      "layer.shape = torch.Size([512]) has 512 parameters\n",
      "layer.shape = torch.Size([2048, 512, 1, 1]) has 1048576 parameters\n",
      "layer.shape = torch.Size([2048]) has 2048 parameters\n",
      "layer.shape = torch.Size([2048]) has 2048 parameters\n",
      "layer.shape = torch.Size([10, 2048]) has 20480 parameters\n",
      "layer.shape = torch.Size([10]) has 10 parameters\n"
     ]
    }
   ],
   "source": [
    "# Count number of layer-wise parameters and total parameters-\n",
    "tot_params = 0\n",
    "for param in model.parameters():\n",
    "    print(f\"layer.shape = {param.shape} has {param.nelement()} parameters\")\n",
    "    tot_params += param.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bp9X8qnOsY1t",
    "outputId": "161ec2b1-37d8-4676-dcf8-aa96cf9bfc3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in ResNet-50 CNN for CIFAR-10 = 23520842\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of parameters in ResNet-50 CNN for CIFAR-10 = {tot_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5WdKiWsLsZYN"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Print layer names-\n",
    "for layer in model.state_dict().keys():\n",
    "    print(f\"{layer} has dimension = {model.state_dict()[layer].shape}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1jFfuJiscmN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "O_WUBfzFscrc"
   },
   "outputs": [],
   "source": [
    "# Save random initial weights-\n",
    "torch.save(model.state_dict(), 'ResNet50_random_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gdgada5J5mIR",
    "outputId": "4ad9e79d-a499-4831-c030-219544802967"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load randomly initialised weights-\n",
    "# model.load_state_dict(torch.load('ResNet50_random_weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGUR1m04stvU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "OozQ7SF4st4U"
   },
   "outputs": [],
   "source": [
    "# Define loss function and optimizer-\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9, weight_decay = 5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1NXRjN5is4eF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "stjVD5uus0xW"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def train_step(model, train_loader):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "    for batch, (images, labels) in enumerate(train_loader):\n",
    "        # Attempt to push to GPU if available\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # print(f\"batch # = {batch}\")\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        # Forward pass-\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute loss-\n",
    "        J = loss(outputs, labels)\n",
    "\n",
    "        # Backward pass-\n",
    "        optimizer.zero_grad()   # empty accumulated gradients\n",
    "\n",
    "        J.backward()    # perform backpropagation\n",
    "\n",
    "        # Updates parameters-\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Compute model's performance statistics-\n",
    "        running_loss += J.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(predicted == labels.data)\n",
    "\n",
    "        '''\n",
    "        # Print information every 100 steps-\n",
    "        if (batch + 1) % 100 == 0:\n",
    "            print(f\"epoch {epoch + 1}/{num_epochs}, step {batch + 1}/{num_training_steps}, loss = {J.item():.4f}\")\n",
    "        '''\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = (running_corrects.double() / len(train_dataset)) * 100\n",
    "    \n",
    "    return epoch_loss, epoch_acc.cpu().numpy()\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "0DyTwwRhcOeY"
   },
   "outputs": [],
   "source": [
    "def train_model_progress(model, train_loader):\n",
    "    '''\n",
    "    Function to perform one epoch of training by using 'train_loader'.\n",
    "    Returns loss and number of correct predictions for this epoch.\n",
    "    '''\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    with tqdm(train_loader, unit = 'batch') as tepoch:\n",
    "        for images, labels in tepoch:\n",
    "            tepoch.set_description(f\"Training: \")\n",
    "            \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Get model predictions-\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Compute loss-\n",
    "            J = loss(outputs, labels)\n",
    "            \n",
    "            # Empty accumulated gradients-\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Perform backprop-\n",
    "            J.backward()\n",
    "            \n",
    "            # Update parameters-\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Compute model's performance statistics-\n",
    "            running_loss += J.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            running_corrects += torch.sum(predicted == labels.data)\n",
    "            \n",
    "            tepoch.set_postfix(\n",
    "                loss = running_loss / len(train_dataset),\n",
    "                accuracy = (running_corrects.double().cpu().numpy() / len(train_dataset)) * 100\n",
    "            )\n",
    "            \n",
    "    \n",
    "    train_loss = running_loss / len(train_dataset)\n",
    "    train_acc = (running_corrects.double() / len(train_dataset)) * 100\n",
    "    \n",
    "\n",
    "    # return running_loss, running_corrects\n",
    "    return train_loss, train_acc.cpu().numpy()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCq8FAdOckD6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "lHUDSkX8s8Od"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def validate_step(model, test_loader):\n",
    "    total, correct = 0, 0\n",
    "    running_loss_val = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "\n",
    "            # Place features (images) and targets (labels) to GPU-\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Set model to evaluation mode-\n",
    "            model.eval()\n",
    "    \n",
    "            # Make predictions using trained model-\n",
    "            outputs = model(images)\n",
    "            _, y_pred = torch.max(outputs, 1)\n",
    "\n",
    "            # Compute validation loss-\n",
    "            J_val = loss(outputs, labels)\n",
    "\n",
    "            running_loss_val += J_val.item() * labels.size(0)\n",
    "    \n",
    "            # Total number of labels-\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Total number of correct predictions-\n",
    "            correct += (y_pred == labels).sum()\n",
    "\n",
    "    epoch_val_loss = running_loss_val / len(test_dataset)\n",
    "    val_acc = 100 * (correct / total)\n",
    "\n",
    "    return epoch_val_loss, val_acc.cpu().numpy()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "YymBc1UtcnEA"
   },
   "outputs": [],
   "source": [
    "def test_model_progress(model, test_loader):\n",
    "    total = 0.0\n",
    "    correct = 0.0\n",
    "    running_loss_val = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_loader, unit = 'batch') as tepoch:\n",
    "            for images, labels in tepoch:\n",
    "                tepoch.set_description(f\"Validation: \")\n",
    "                \n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Set model to evaluation mode-\n",
    "                model.eval()\n",
    "            \n",
    "                # Predict using trained model-\n",
    "                outputs = model(images)\n",
    "                _, y_pred = torch.max(outputs, 1)\n",
    "                \n",
    "                # Compute validation loss-\n",
    "                J_val = loss(outputs, labels)\n",
    "                \n",
    "                running_loss_val += J_val.item() * labels.size(0)\n",
    "    \n",
    "                # Total number of labels-\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total number of correct predictions-\n",
    "                correct += (y_pred == labels).sum()\n",
    "                \n",
    "                tepoch.set_postfix(\n",
    "                    val_loss = running_loss_val / len(test_dataset),\n",
    "                    val_acc = 100 * (correct.cpu().numpy() / total)\n",
    "                )\n",
    "            \n",
    "        \n",
    "    # return (running_loss_val, correct, total)\n",
    "    val_loss = running_loss_val / len(test_dataset)\n",
    "    val_acc = (correct / total) * 100\n",
    "\n",
    "    return val_loss, val_acc.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "3BGHQY9YcuqJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n5OVpgCicu1i",
    "outputId": "20d63dcf-26b5-4958-96f7-c134ef794420"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [04:08<00:00,  1.57batch/s, accuracy=45.5, loss=1.47]\n"
     ]
    }
   ],
   "source": [
    "# Sanity check-\n",
    "train_loss, train_acc = train_model_progress(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AP1HjJC6c7nS",
    "outputId": "44c7dcd7-c693-4904-9328-d6cf916ab75f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: : 100%|| 79/79 [00:08<00:00,  9.06batch/s, val_acc=56.6, val_loss=1.21]\n"
     ]
    }
   ],
   "source": [
    "# Sanity check-\n",
    "val_loss, val_acc = test_model_progress(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RgTblttjdFXu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gw4-9h2vibj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDIK4_3W8L95"
   },
   "source": [
    "### Train model _without_ learning rate scheduler, using early-stopping criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "NOytz_v9tPxV"
   },
   "outputs": [],
   "source": [
    "# Initialize parameters for Early Stopping manual implementation-\n",
    "best_val_loss = 100\n",
    "loc_patience = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "SbXa8wiR6OHv"
   },
   "outputs": [],
   "source": [
    "# User input parameters for Early Stopping in manual implementation-\n",
    "minimum_delta = 0.001\n",
    "patience = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "K1oqH6x25YFS"
   },
   "outputs": [],
   "source": [
    "training_history = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "qYVpzJ0_h97Z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "G7E2MwUJh-Dv",
    "outputId": "e29a407f-e25c-4b77-9ee2-3bd839eb7952"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:44<00:00,  1.74batch/s, accuracy=33, loss=1.88]    \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.18batch/s, val_acc=40, val_loss=1.68]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 1 training loss = 1.8763, training accuracy = 32.97%, val_loss = 1.6837, val_accuracy = 40.00% & LR = 0.0100\n",
      "\n",
      "Saving model with lowest val_loss = 1.6837\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:46<00:00,  1.73batch/s, accuracy=53.3, loss=1.29] \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.15batch/s, val_acc=58, val_loss=1.15]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 2 training loss = 1.2924, training accuracy = 53.31%, val_loss = 1.1474, val_accuracy = 58.01% & LR = 0.0100\n",
      "\n",
      "Saving model with lowest val_loss = 1.1474\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:46<00:00,  1.73batch/s, accuracy=65.1, loss=0.986] \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.03batch/s, val_acc=65.8, val_loss=0.985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 3 training loss = 0.9865, training accuracy = 65.15%, val_loss = 0.9849, val_accuracy = 65.84% & LR = 0.0100\n",
      "\n",
      "Saving model with lowest val_loss = 0.9849\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:47<00:00,  1.72batch/s, accuracy=72.8, loss=0.782] \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  4.99batch/s, val_acc=70.9, val_loss=0.881]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 4 training loss = 0.7820, training accuracy = 72.76%, val_loss = 0.8809, val_accuracy = 70.89% & LR = 0.0100\n",
      "\n",
      "Saving model with lowest val_loss = 0.8809\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:54<00:00,  1.67batch/s, accuracy=77.3, loss=0.658] \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.00batch/s, val_acc=75.4, val_loss=0.749] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 5 training loss = 0.6584, training accuracy = 77.28%, val_loss = 0.7486, val_accuracy = 75.44% & LR = 0.0100\n",
      "\n",
      "Saving model with lowest val_loss = 0.7486\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:31<00:00,  1.85batch/s, accuracy=80.1, loss=0.57]  \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.10batch/s, val_acc=77, val_loss=0.701]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 6 training loss = 0.5698, training accuracy = 80.13%, val_loss = 0.7006, val_accuracy = 77.02% & LR = 0.0100\n",
      "\n",
      "Saving model with lowest val_loss = 0.7006\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:46<00:00,  1.72batch/s, accuracy=82.4, loss=0.51]  \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.22batch/s, val_acc=80.5, val_loss=0.59]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 7 training loss = 0.5105, training accuracy = 82.36%, val_loss = 0.5904, val_accuracy = 80.48% & LR = 0.0100\n",
      "\n",
      "Saving model with lowest val_loss = 0.5904\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:45<00:00,  1.73batch/s, accuracy=84.1, loss=0.46]  \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.14batch/s, val_acc=83.4, val_loss=0.503] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 8 training loss = 0.4598, training accuracy = 84.14%, val_loss = 0.5028, val_accuracy = 83.41% & LR = 0.0100\n",
      "\n",
      "Saving model with lowest val_loss = 0.5028\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:45<00:00,  1.74batch/s, accuracy=85.6, loss=0.419] \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.11batch/s, val_acc=81.6, val_loss=0.576] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 9 training loss = 0.4188, training accuracy = 85.56%, val_loss = 0.5765, val_accuracy = 81.64% & LR = 0.0100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:55<00:00,  1.66batch/s, accuracy=86.7, loss=0.38]  \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.13batch/s, val_acc=81.9, val_loss=0.559] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 10 training loss = 0.3800, training accuracy = 86.67%, val_loss = 0.5593, val_accuracy = 81.91% & LR = 0.0100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:46<00:00,  1.73batch/s, accuracy=87.7, loss=0.357] \n",
      "Validation: : 100%|| 79/79 [00:14<00:00,  5.30batch/s, val_acc=83.5, val_loss=0.55]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 11 training loss = 0.3567, training accuracy = 87.68%, val_loss = 0.5501, val_accuracy = 83.51% & LR = 0.0100\n",
      "\n",
      "\n",
      "'EarlyStopping' called!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    if loc_patience >= patience:\n",
    "        print(\"\\n'EarlyStopping' called!\\n\")\n",
    "        break\n",
    "\n",
    "    train_loss, train_acc = train_model_progress(model, train_loader)\n",
    "    val_loss, val_acc = test_model_progress(model, test_loader)\n",
    "    \n",
    "    print(f\"\\nepoch: {epoch + 1} training loss = {train_loss:.4f}, \"\n",
    "          f\"training accuracy = {train_acc:.2f}%, val_loss = {val_loss:.4f}\"\n",
    "          f\", val_accuracy = {val_acc:.2f}% & \"\n",
    "          f\"LR = {optimizer.param_groups[0]['lr']:.4f}\\n\")\n",
    "    \n",
    "    training_history[epoch + 1] = {\n",
    "        'loss': train_loss, 'acc': train_acc,\n",
    "        'val_loss': val_loss, 'val_acc': val_acc,\n",
    "        'lr': optimizer.param_groups[0]['lr']\n",
    "    }\n",
    "\n",
    "    '''\n",
    "    # Save best weights achieved until now-\n",
    "    if (val_loss < best_val_loss):    \n",
    "        # update 'best_val_loss' variable to lowest loss encountered so far-\n",
    "        best_val_loss = val_loss\n",
    "\n",
    "        print(f\"Saving model with lowest val_loss = {val_loss:.4f}\\n\")\n",
    "        \n",
    "        # Save trained model with 'best' validation accuracy-\n",
    "        torch.save(model.state_dict(), \"VGG18_best_model.pth\")\n",
    "    '''\n",
    "\n",
    "    # Code for manual Early Stopping:\n",
    "    # if np.abs(val_loss < best_val_loss) >= minimum_delta:\n",
    "    if (val_loss < best_val_loss) and \\\n",
    "    (np.abs(val_loss - best_val_loss) >= minimum_delta):\n",
    "\n",
    "        # update 'best_val_loss' variable to lowest loss encountered so far-\n",
    "        best_val_loss = val_loss\n",
    "        \n",
    "        # reset 'loc_patience' variable-\n",
    "        loc_patience = 0\n",
    "\n",
    "        print(f\"Saving model with lowest val_loss = {val_loss:.4f}\\n\")\n",
    "        \n",
    "        # Save trained model with 'best' validation accuracy-\n",
    "        torch.save(model.state_dict(), \"ResNet50_best_model.pth\")\n",
    "        \n",
    "    else:  # there is no improvement in monitored metric 'val_loss'\n",
    "        loc_patience += 1  # number of epochs without any improvement\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HW6eI7f-4W9R"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X3dcgzOwii1g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qXesVlcAtj_U",
    "outputId": "fa906693-19de-4aa6-fe1c-b4d197346f26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "COrZUx-_P6Zx",
    "outputId": "66e63461-5f0b-436b-b4fd-5615ef5c769b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'acc', 'val_loss', 'val_acc', 'lr'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_history[2].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D0ZrNvFoP_fZ",
    "outputId": "d1441b8f-507c-4044-93ae-c4a775c61fb7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(58.01, dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_history[2]['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "loPQquhpQC9Z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "yJkajjh9QGKK"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "0G1TaNnnQHDI"
   },
   "outputs": [],
   "source": [
    "with open(\"ResNet50_earlystopping_training_history.pkl\", \"wb\") as file:\n",
    "    pickle.dump(training_history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DiAv9vgBQigh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: : 100%|| 79/79 [00:15<00:00,  4.95batch/s, val_acc=83.5, val_loss=0.55]  \n"
     ]
    }
   ],
   "source": [
    "# Get model metrics at last epoch-\n",
    "val_loss, val_acc = test_model_progress(model = model, test_loader = test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet-50 (Early Stopping) val_acc = 83.51%\n"
     ]
    }
   ],
   "source": [
    "print(f\"ResNet-50 (Early Stopping) val_acc = {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "MuBmbE6CQuPO"
   },
   "outputs": [],
   "source": [
    "# Save trained weights-\n",
    "torch.save(model.state_dict(), 'ResNet50_earlystopping_trained_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tSOrFC5gQz9b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h0Fbk-V3Q0H8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WC2enWfbT5wt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model with _learning rate scheduler_\n",
    "\n",
    "- Training dataset = 50000, batch size = 128, number of training steps/iterations = 50000 / 128 = 391\n",
    "\n",
    "- Initial learning rate warmup: 391 x 10 = 3910 steps or, 10 epochs at LR = 0.1\n",
    "\n",
    "- Until 25th epoch or, 9775 steps use LR = 0.1\n",
    "\n",
    "- From 26th epoch until 40th epoch or, 15640 steps use LR = 0.01\n",
    "\n",
    "- From 41st epoch until 50th epoch or, 19550 steps use LR = 0.001\n",
    "\n",
    "- From 51st epoch until 60th epoch use LR = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundaries = [9775, 15640, 19550]\n",
    "values = [0.1, 0.01, 0.001, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a ResNet-50 model-\n",
    "model = ResNet50(img_channels = 3, num_channels = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load random weights from before-\n",
    "model.load_state_dict(torch.load('ResNet50_random_weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "OozQ7SF4st4U"
   },
   "outputs": [],
   "source": [
    "# Define loss function and optimizer-\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.0, momentum = 0.9, weight_decay = 5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check-\n",
    "optimizer.param_groups[0]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_function(step, boundaries = [9775, 15640, 19550], values = [0.1, 0.01, 0.001, 0.0001]):\n",
    "    '''\n",
    "    1 epoch has 391 steps/iterations using batch size used above.\n",
    "    \n",
    "    Until 25th epochs, or 25 x 391 = 9775 steps, use lr = 0.1\n",
    "    From 26th epoch until 40th epoch, or 15640 steps use LR = 0.01\n",
    "    From 41st epoch until 50th epoch or, 19550 steps use LR = 0.001\n",
    "    From 51st epoch until 60th epoch use LR = 0.0001\n",
    "    '''\n",
    "    \n",
    "    for idx, bound in enumerate(boundaries):\n",
    "        if step < bound:\n",
    "            return values[idx]\n",
    "\n",
    "    return values[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class schedule():\n",
    "\n",
    "    def __init__(self, initial_learning_rate = 0.1, warmup_steps = 1000, decay_func = None):\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.decay_func = decay_func\n",
    "        self.warmup_step_size = initial_learning_rate/warmup_steps\n",
    "        self.current_lr = 0\n",
    "\n",
    "    def get_lr(self, step):\n",
    "        if step == 0:\n",
    "            return self.current_lr\n",
    "        elif step <= self.warmup_steps:\n",
    "            self.current_lr+=self.warmup_step_size\n",
    "            return self.current_lr\n",
    "        elif step > self.warmup_steps:\n",
    "            if self.decay_func:\n",
    "                return self.decay_func(step)\n",
    "        else:\n",
    "            return self.current_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 391 x 10 = 3910 steps (or, 10 epochs) is learning rate warmup\n",
    "custom_lr_scheduler = schedule(\n",
    "    initial_learning_rate = 0.1, warmup_steps = 3910,\n",
    "    decay_func = decay_function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "0DyTwwRhcOeY"
   },
   "outputs": [],
   "source": [
    "def train_model_progress(model, train_loader):\n",
    "    '''\n",
    "    Function to perform one epoch of training by using 'train_loader'.\n",
    "    Returns loss and number of correct predictions for this epoch.\n",
    "    '''\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    with tqdm(train_loader, unit = 'batch') as tepoch:\n",
    "        for images, labels in tepoch:\n",
    "            tepoch.set_description(f\"Training: \")\n",
    "            \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Get model predictions-\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Compute loss-\n",
    "            J = loss(outputs, labels)\n",
    "            \n",
    "            # Empty accumulated gradients-\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Perform backprop-\n",
    "            J.backward()\n",
    "            \n",
    "            # Update parameters-\n",
    "            optimizer.step()\n",
    "            \n",
    "            global step\n",
    "            optimizer.param_groups[0]['lr'] = custom_lr_scheduler.get_lr(step)\n",
    "\n",
    "            step += 1\n",
    "            \n",
    "            # Compute model's performance statistics-\n",
    "            running_loss += J.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            running_corrects += torch.sum(predicted == labels.data)\n",
    "            \n",
    "            tepoch.set_postfix(\n",
    "                loss = running_loss / len(train_dataset),\n",
    "                accuracy = (running_corrects.double().cpu().numpy() / len(train_dataset)) * 100\n",
    "            )\n",
    "            \n",
    "    \n",
    "    train_loss = running_loss / len(train_dataset)\n",
    "    train_acc = (running_corrects.double() / len(train_dataset)) * 100\n",
    "    \n",
    "\n",
    "    # return running_loss, running_corrects\n",
    "    return train_loss, train_acc.cpu().numpy()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCq8FAdOckD6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "YymBc1UtcnEA"
   },
   "outputs": [],
   "source": [
    "def test_model_progress(model, test_loader):\n",
    "    total = 0.0\n",
    "    correct = 0.0\n",
    "    running_loss_val = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_loader, unit = 'batch') as tepoch:\n",
    "            for images, labels in tepoch:\n",
    "                tepoch.set_description(f\"Validation: \")\n",
    "                \n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Set model to evaluation mode-\n",
    "                model.eval()\n",
    "            \n",
    "                # Predict using trained model-\n",
    "                outputs = model(images)\n",
    "                _, y_pred = torch.max(outputs, 1)\n",
    "                \n",
    "                # Compute validation loss-\n",
    "                J_val = loss(outputs, labels)\n",
    "                \n",
    "                running_loss_val += J_val.item() * labels.size(0)\n",
    "    \n",
    "                # Total number of labels-\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total number of correct predictions-\n",
    "                correct += (y_pred == labels).sum()\n",
    "                \n",
    "                tepoch.set_postfix(\n",
    "                    val_loss = running_loss_val / len(test_dataset),\n",
    "                    val_acc = 100 * (correct.cpu().numpy() / total)\n",
    "                )\n",
    "            \n",
    "        \n",
    "    # return (running_loss_val, correct, total)\n",
    "    val_loss = running_loss_val / len(test_dataset)\n",
    "    val_acc = (correct / total) * 100\n",
    "\n",
    "    return val_loss, val_acc.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (layer1): Sequential(\n",
       "    (0): ResNet_block(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (identity_downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResNet_block(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): ResNet_block(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): ResNet_block(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (identity_downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResNet_block(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): ResNet_block(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (3): ResNet_block(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): ResNet_block(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (identity_downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResNet_block(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): ResNet_block(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (3): ResNet_block(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (4): ResNet_block(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (5): ResNet_block(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): ResNet_block(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (identity_downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResNet_block(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): ResNet_block(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Place model on GPU (if available)-\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history_lr_scheduler = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "NOytz_v9tPxV"
   },
   "outputs": [],
   "source": [
    "# Initialize parameters for Early Stopping manual implementation-\n",
    "best_val_loss = 100\n",
    "loc_patience = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:55<00:00,  1.66batch/s, accuracy=29.8, loss=1.94]  \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.20batch/s, val_acc=43.2, val_loss=1.6]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 1 training loss = 1.9413, training accuracy = 29.83%, val_loss = 1.6045, val_accuracy = 43.16% & LR = 0.0100\n",
      "\n",
      "Saving model with lowest val_loss = 1.6045\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:56<00:00,  1.65batch/s, accuracy=52.8, loss=1.32]  \n",
      "Validation: : 100%|| 79/79 [00:14<00:00,  5.29batch/s, val_acc=60.2, val_loss=1.16] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 2 training loss = 1.3228, training accuracy = 52.82%, val_loss = 1.1645, val_accuracy = 60.22% & LR = 0.0200\n",
      "\n",
      "Saving model with lowest val_loss = 1.1645\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:55<00:00,  1.66batch/s, accuracy=65.1, loss=0.993] \n",
      "Validation: : 100%|| 79/79 [00:14<00:00,  5.47batch/s, val_acc=63.7, val_loss=1.22] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 3 training loss = 0.9932, training accuracy = 65.08%, val_loss = 1.2222, val_accuracy = 63.75% & LR = 0.0300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:57<00:00,  1.65batch/s, accuracy=71.8, loss=0.81]  \n",
      "Validation: : 100%|| 79/79 [00:16<00:00,  4.93batch/s, val_acc=75.3, val_loss=0.731] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 4 training loss = 0.8103, training accuracy = 71.80%, val_loss = 0.7305, val_accuracy = 75.33% & LR = 0.0400\n",
      "\n",
      "Saving model with lowest val_loss = 0.7305\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [04:04<00:00,  1.60batch/s, accuracy=76.6, loss=0.679] \n",
      "Validation: : 100%|| 79/79 [00:28<00:00,  2.77batch/s, val_acc=74.9, val_loss=0.761] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 5 training loss = 0.6791, training accuracy = 76.59%, val_loss = 0.7615, val_accuracy = 74.88% & LR = 0.0500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [04:23<00:00,  1.48batch/s, accuracy=79.1, loss=0.61]  \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.20batch/s, val_acc=77.9, val_loss=0.672] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 6 training loss = 0.6103, training accuracy = 79.05%, val_loss = 0.6725, val_accuracy = 77.92% & LR = 0.0600\n",
      "\n",
      "Saving model with lowest val_loss = 0.6725\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [04:09<00:00,  1.57batch/s, accuracy=80.6, loss=0.558] \n",
      "Validation: : 100%|| 79/79 [00:16<00:00,  4.90batch/s, val_acc=73.1, val_loss=0.844]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 7 training loss = 0.5578, training accuracy = 80.65%, val_loss = 0.8439, val_accuracy = 73.13% & LR = 0.0700\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [04:02<00:00,  1.61batch/s, accuracy=81.3, loss=0.546] \n",
      "Validation: : 100%|| 79/79 [00:16<00:00,  4.84batch/s, val_acc=79.5, val_loss=0.595] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 8 training loss = 0.5459, training accuracy = 81.30%, val_loss = 0.5951, val_accuracy = 79.48% & LR = 0.0800\n",
      "\n",
      "Saving model with lowest val_loss = 0.5951\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:57<00:00,  1.65batch/s, accuracy=81.9, loss=0.526] \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  4.97batch/s, val_acc=77.2, val_loss=0.666] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 9 training loss = 0.5255, training accuracy = 81.90%, val_loss = 0.6662, val_accuracy = 77.25% & LR = 0.0900\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:58<00:00,  1.64batch/s, accuracy=82.4, loss=0.514] \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  4.97batch/s, val_acc=77.8, val_loss=0.683] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 10 training loss = 0.5145, training accuracy = 82.35%, val_loss = 0.6832, val_accuracy = 77.80% & LR = 0.1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:54<00:00,  1.67batch/s, accuracy=82.8, loss=0.502] \n",
      "Validation: : 100%|| 79/79 [00:16<00:00,  4.74batch/s, val_acc=77.5, val_loss=0.665] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 11 training loss = 0.5015, training accuracy = 82.80%, val_loss = 0.6655, val_accuracy = 77.48% & LR = 0.1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [04:00<00:00,  1.62batch/s, accuracy=83.5, loss=0.48]  \n",
      "Validation: : 100%|| 79/79 [00:16<00:00,  4.73batch/s, val_acc=79.2, val_loss=0.598] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 12 training loss = 0.4799, training accuracy = 83.49%, val_loss = 0.5979, val_accuracy = 79.20% & LR = 0.1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:55<00:00,  1.66batch/s, accuracy=83.9, loss=0.468] \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  4.99batch/s, val_acc=80.7, val_loss=0.554] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 13 training loss = 0.4681, training accuracy = 83.91%, val_loss = 0.5535, val_accuracy = 80.74% & LR = 0.1000\n",
      "\n",
      "Saving model with lowest val_loss = 0.5535\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:53<00:00,  1.68batch/s, accuracy=84.7, loss=0.449] \n",
      "Validation: : 100%|| 79/79 [00:14<00:00,  5.35batch/s, val_acc=78.8, val_loss=0.649] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 14 training loss = 0.4488, training accuracy = 84.66%, val_loss = 0.6490, val_accuracy = 78.76% & LR = 0.1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:55<00:00,  1.66batch/s, accuracy=84.5, loss=0.45]  \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.13batch/s, val_acc=76.1, val_loss=0.747] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 15 training loss = 0.4504, training accuracy = 84.45%, val_loss = 0.7467, val_accuracy = 76.11% & LR = 0.1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:53<00:00,  1.67batch/s, accuracy=85, loss=0.441]   \n",
      "Validation: : 100%|| 79/79 [00:16<00:00,  4.84batch/s, val_acc=78.1, val_loss=0.643] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 16 training loss = 0.4413, training accuracy = 84.95%, val_loss = 0.6433, val_accuracy = 78.11% & LR = 0.1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:53<00:00,  1.67batch/s, accuracy=85, loss=0.438]   \n",
      "Validation: : 100%|| 79/79 [00:14<00:00,  5.29batch/s, val_acc=79.5, val_loss=0.62]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 17 training loss = 0.4380, training accuracy = 84.98%, val_loss = 0.6196, val_accuracy = 79.50% & LR = 0.1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:55<00:00,  1.66batch/s, accuracy=85.3, loss=0.428] \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.08batch/s, val_acc=80.8, val_loss=0.582] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 18 training loss = 0.4282, training accuracy = 85.33%, val_loss = 0.5823, val_accuracy = 80.82% & LR = 0.1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:56<00:00,  1.65batch/s, accuracy=85.2, loss=0.43]   \n",
      "Validation: : 100%|| 79/79 [00:16<00:00,  4.90batch/s, val_acc=80.5, val_loss=0.594] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 19 training loss = 0.4304, training accuracy = 85.19%, val_loss = 0.5942, val_accuracy = 80.50% & LR = 0.1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:52<00:00,  1.68batch/s, accuracy=85.5, loss=0.425] \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.02batch/s, val_acc=81, val_loss=0.567]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 20 training loss = 0.4246, training accuracy = 85.48%, val_loss = 0.5668, val_accuracy = 81.01% & LR = 0.1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:53<00:00,  1.67batch/s, accuracy=85.6, loss=0.422] \n",
      "Validation: : 100%|| 79/79 [00:16<00:00,  4.80batch/s, val_acc=82, val_loss=0.542]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 21 training loss = 0.4215, training accuracy = 85.56%, val_loss = 0.5417, val_accuracy = 82.01% & LR = 0.1000\n",
      "\n",
      "Saving model with lowest val_loss = 0.5417\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:55<00:00,  1.66batch/s, accuracy=85.8, loss=0.413]  \n",
      "Validation: : 100%|| 79/79 [00:14<00:00,  5.60batch/s, val_acc=77, val_loss=0.777]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 22 training loss = 0.4129, training accuracy = 85.81%, val_loss = 0.7774, val_accuracy = 77.04% & LR = 0.1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:53<00:00,  1.67batch/s, accuracy=85.7, loss=0.418] \n",
      "Validation: : 100%|| 79/79 [00:16<00:00,  4.88batch/s, val_acc=81.9, val_loss=0.545] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 23 training loss = 0.4183, training accuracy = 85.71%, val_loss = 0.5450, val_accuracy = 81.89% & LR = 0.1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:53<00:00,  1.68batch/s, accuracy=86, loss=0.409]   \n",
      "Validation: : 100%|| 79/79 [00:17<00:00,  4.61batch/s, val_acc=80.2, val_loss=0.637] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 24 training loss = 0.4093, training accuracy = 85.97%, val_loss = 0.6368, val_accuracy = 80.15% & LR = 0.1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:53<00:00,  1.67batch/s, accuracy=86, loss=0.407]    \n",
      "Validation: : 100%|| 79/79 [00:14<00:00,  5.58batch/s, val_acc=83.9, val_loss=0.489] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 25 training loss = 0.4068, training accuracy = 86.02%, val_loss = 0.4895, val_accuracy = 83.90% & LR = 0.1000\n",
      "\n",
      "Saving model with lowest val_loss = 0.4895\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:53<00:00,  1.68batch/s, accuracy=92.2, loss=0.231] \n",
      "Validation: : 100%|| 79/79 [00:16<00:00,  4.91batch/s, val_acc=91.3, val_loss=0.245] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 26 training loss = 0.2311, training accuracy = 92.21%, val_loss = 0.2449, val_accuracy = 91.30% & LR = 0.0100\n",
      "\n",
      "Saving model with lowest val_loss = 0.2449\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:54<00:00,  1.67batch/s, accuracy=94.1, loss=0.176] \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.01batch/s, val_acc=91.9, val_loss=0.233] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 27 training loss = 0.1757, training accuracy = 94.05%, val_loss = 0.2332, val_accuracy = 91.87% & LR = 0.0100\n",
      "\n",
      "Saving model with lowest val_loss = 0.2332\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:52<00:00,  1.68batch/s, accuracy=94.7, loss=0.157]  \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.17batch/s, val_acc=92.1, val_loss=0.226] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 28 training loss = 0.1571, training accuracy = 94.73%, val_loss = 0.2262, val_accuracy = 92.07% & LR = 0.0100\n",
      "\n",
      "Saving model with lowest val_loss = 0.2262\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:52<00:00,  1.68batch/s, accuracy=95.2, loss=0.142]  \n",
      "Validation: : 100%|| 79/79 [00:14<00:00,  5.27batch/s, val_acc=92.2, val_loss=0.228] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 29 training loss = 0.1418, training accuracy = 95.16%, val_loss = 0.2278, val_accuracy = 92.24% & LR = 0.0100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:55<00:00,  1.66batch/s, accuracy=95.4, loss=0.132]  \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  4.97batch/s, val_acc=92, val_loss=0.235]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 30 training loss = 0.1319, training accuracy = 95.40%, val_loss = 0.2351, val_accuracy = 91.95% & LR = 0.0100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:52<00:00,  1.68batch/s, accuracy=95.9, loss=0.12]   \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.01batch/s, val_acc=92.2, val_loss=0.233] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 31 training loss = 0.1200, training accuracy = 95.91%, val_loss = 0.2331, val_accuracy = 92.22% & LR = 0.0100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:55<00:00,  1.66batch/s, accuracy=96.3, loss=0.108]  \n",
      "Validation: : 100%|| 79/79 [00:14<00:00,  5.57batch/s, val_acc=92.2, val_loss=0.241] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 32 training loss = 0.1079, training accuracy = 96.29%, val_loss = 0.2415, val_accuracy = 92.15% & LR = 0.0100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:56<00:00,  1.65batch/s, accuracy=96.4, loss=0.104]  \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.18batch/s, val_acc=92.4, val_loss=0.237] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 33 training loss = 0.1039, training accuracy = 96.42%, val_loss = 0.2372, val_accuracy = 92.43% & LR = 0.0100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:52<00:00,  1.68batch/s, accuracy=96.6, loss=0.0978] \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.04batch/s, val_acc=91.8, val_loss=0.251] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 34 training loss = 0.0978, training accuracy = 96.64%, val_loss = 0.2506, val_accuracy = 91.82% & LR = 0.0100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:56<00:00,  1.66batch/s, accuracy=96.8, loss=0.0932] \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.05batch/s, val_acc=91.8, val_loss=0.264] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 35 training loss = 0.0932, training accuracy = 96.79%, val_loss = 0.2635, val_accuracy = 91.83% & LR = 0.0100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:54<00:00,  1.67batch/s, accuracy=97, loss=0.0867]   \n",
      "Validation: : 100%|| 79/79 [00:16<00:00,  4.81batch/s, val_acc=92.4, val_loss=0.245] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 36 training loss = 0.0867, training accuracy = 97.01%, val_loss = 0.2450, val_accuracy = 92.41% & LR = 0.0100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:52<00:00,  1.69batch/s, accuracy=97, loss=0.0855]   \n",
      "Validation: : 100%|| 79/79 [00:16<00:00,  4.92batch/s, val_acc=92.5, val_loss=0.249] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 37 training loss = 0.0855, training accuracy = 96.97%, val_loss = 0.2490, val_accuracy = 92.46% & LR = 0.0100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:53<00:00,  1.67batch/s, accuracy=97.2, loss=0.0803] \n",
      "Validation: : 100%|| 79/79 [00:14<00:00,  5.36batch/s, val_acc=92.4, val_loss=0.262] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 38 training loss = 0.0803, training accuracy = 97.20%, val_loss = 0.2616, val_accuracy = 92.40% & LR = 0.0100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:53<00:00,  1.67batch/s, accuracy=97.2, loss=0.0809] \n",
      "Validation: : 100%|| 79/79 [00:17<00:00,  4.59batch/s, val_acc=92.3, val_loss=0.259] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 39 training loss = 0.0809, training accuracy = 97.16%, val_loss = 0.2586, val_accuracy = 92.29% & LR = 0.0100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:53<00:00,  1.67batch/s, accuracy=97.5, loss=0.0749] \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  4.94batch/s, val_acc=92.2, val_loss=0.259] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 40 training loss = 0.0749, training accuracy = 97.46%, val_loss = 0.2594, val_accuracy = 92.21% & LR = 0.0100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:52<00:00,  1.68batch/s, accuracy=98.4, loss=0.0496] \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.10batch/s, val_acc=93.5, val_loss=0.218] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 41 training loss = 0.0496, training accuracy = 98.36%, val_loss = 0.2185, val_accuracy = 93.50% & LR = 0.0010\n",
      "\n",
      "Saving model with lowest val_loss = 0.2185\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:53<00:00,  1.67batch/s, accuracy=99, loss=0.0368]   \n",
      "Validation: : 100%|| 79/79 [00:17<00:00,  4.49batch/s, val_acc=93.7, val_loss=0.215] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 42 training loss = 0.0368, training accuracy = 98.95%, val_loss = 0.2149, val_accuracy = 93.72% & LR = 0.0010\n",
      "\n",
      "Saving model with lowest val_loss = 0.2149\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:55<00:00,  1.66batch/s, accuracy=99.1, loss=0.0326] \n",
      "Validation: : 100%|| 79/79 [00:16<00:00,  4.65batch/s, val_acc=93.7, val_loss=0.214] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 43 training loss = 0.0326, training accuracy = 99.06%, val_loss = 0.2140, val_accuracy = 93.67% & LR = 0.0010\n",
      "\n",
      "Saving model with lowest val_loss = 0.2140\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:53<00:00,  1.67batch/s, accuracy=99.2, loss=0.0294] \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.04batch/s, val_acc=93.8, val_loss=0.214] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 44 training loss = 0.0294, training accuracy = 99.17%, val_loss = 0.2143, val_accuracy = 93.85% & LR = 0.0010\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:51<00:00,  1.69batch/s, accuracy=99.3, loss=0.0264] \n",
      "Validation: : 100%|| 79/79 [00:17<00:00,  4.45batch/s, val_acc=93.6, val_loss=0.216] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 45 training loss = 0.0264, training accuracy = 99.25%, val_loss = 0.2156, val_accuracy = 93.63% & LR = 0.0010\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:54<00:00,  1.67batch/s, accuracy=99.3, loss=0.0253] \n",
      "Validation: : 100%|| 79/79 [00:16<00:00,  4.91batch/s, val_acc=93.8, val_loss=0.217] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 46 training loss = 0.0253, training accuracy = 99.28%, val_loss = 0.2174, val_accuracy = 93.75% & LR = 0.0010\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:53<00:00,  1.68batch/s, accuracy=99.3, loss=0.0239] \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.25batch/s, val_acc=93.8, val_loss=0.22]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 47 training loss = 0.0239, training accuracy = 99.30%, val_loss = 0.2203, val_accuracy = 93.82% & LR = 0.0010\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:55<00:00,  1.66batch/s, accuracy=99.4, loss=0.0225] \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.08batch/s, val_acc=93.8, val_loss=0.223] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 48 training loss = 0.0225, training accuracy = 99.38%, val_loss = 0.2232, val_accuracy = 93.75% & LR = 0.0010\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:56<00:00,  1.65batch/s, accuracy=99.4, loss=0.0216] \n",
      "Validation: : 100%|| 79/79 [00:16<00:00,  4.84batch/s, val_acc=93.8, val_loss=0.225] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 49 training loss = 0.0216, training accuracy = 99.40%, val_loss = 0.2248, val_accuracy = 93.77% & LR = 0.0010\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:53<00:00,  1.68batch/s, accuracy=99.5, loss=0.0191] \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.01batch/s, val_acc=93.8, val_loss=0.226] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 50 training loss = 0.0191, training accuracy = 99.51%, val_loss = 0.2256, val_accuracy = 93.80% & LR = 0.0010\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:52<00:00,  1.68batch/s, accuracy=99.5, loss=0.0193] \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.23batch/s, val_acc=94, val_loss=0.224]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 51 training loss = 0.0193, training accuracy = 99.48%, val_loss = 0.2237, val_accuracy = 93.96% & LR = 0.0001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:57<00:00,  1.65batch/s, accuracy=99.5, loss=0.018]  \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.14batch/s, val_acc=93.8, val_loss=0.224] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 52 training loss = 0.0180, training accuracy = 99.53%, val_loss = 0.2239, val_accuracy = 93.83% & LR = 0.0001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:56<00:00,  1.65batch/s, accuracy=99.5, loss=0.0182] \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.08batch/s, val_acc=93.9, val_loss=0.226] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 53 training loss = 0.0182, training accuracy = 99.53%, val_loss = 0.2258, val_accuracy = 93.91% & LR = 0.0001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:52<00:00,  1.68batch/s, accuracy=99.5, loss=0.0183] \n",
      "Validation: : 100%|| 79/79 [00:14<00:00,  5.29batch/s, val_acc=93.9, val_loss=0.225] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 54 training loss = 0.0183, training accuracy = 99.50%, val_loss = 0.2251, val_accuracy = 93.89% & LR = 0.0001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:56<00:00,  1.66batch/s, accuracy=99.5, loss=0.0178] \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.06batch/s, val_acc=93.8, val_loss=0.226] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 55 training loss = 0.0178, training accuracy = 99.55%, val_loss = 0.2259, val_accuracy = 93.81% & LR = 0.0001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:58<00:00,  1.64batch/s, accuracy=99.5, loss=0.018]  \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.20batch/s, val_acc=93.8, val_loss=0.226] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 56 training loss = 0.0180, training accuracy = 99.51%, val_loss = 0.2258, val_accuracy = 93.82% & LR = 0.0001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:57<00:00,  1.65batch/s, accuracy=99.6, loss=0.0165] \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.07batch/s, val_acc=93.8, val_loss=0.225] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 57 training loss = 0.0165, training accuracy = 99.58%, val_loss = 0.2254, val_accuracy = 93.84% & LR = 0.0001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:52<00:00,  1.68batch/s, accuracy=99.6, loss=0.0168] \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.21batch/s, val_acc=93.9, val_loss=0.225] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 58 training loss = 0.0168, training accuracy = 99.60%, val_loss = 0.2250, val_accuracy = 93.87% & LR = 0.0001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:55<00:00,  1.66batch/s, accuracy=99.6, loss=0.0168] \n",
      "Validation: : 100%|| 79/79 [00:13<00:00,  5.68batch/s, val_acc=93.9, val_loss=0.225] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 59 training loss = 0.0168, training accuracy = 99.57%, val_loss = 0.2253, val_accuracy = 93.94% & LR = 0.0001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:55<00:00,  1.66batch/s, accuracy=99.6, loss=0.0174] \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  4.98batch/s, val_acc=93.8, val_loss=0.226] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 60 training loss = 0.0174, training accuracy = 99.57%, val_loss = 0.2257, val_accuracy = 93.81% & LR = 0.0001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:52<00:00,  1.69batch/s, accuracy=99.5, loss=0.0172] \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.08batch/s, val_acc=93.8, val_loss=0.224] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 61 training loss = 0.0172, training accuracy = 99.53%, val_loss = 0.2241, val_accuracy = 93.84% & LR = 0.0001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:51<00:00,  1.69batch/s, accuracy=99.6, loss=0.0166] \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.21batch/s, val_acc=93.9, val_loss=0.226] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 62 training loss = 0.0166, training accuracy = 99.56%, val_loss = 0.2256, val_accuracy = 93.89% & LR = 0.0001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:52<00:00,  1.68batch/s, accuracy=99.6, loss=0.016]  \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.01batch/s, val_acc=94, val_loss=0.224]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 63 training loss = 0.0160, training accuracy = 99.60%, val_loss = 0.2241, val_accuracy = 93.95% & LR = 0.0001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:52<00:00,  1.68batch/s, accuracy=99.6, loss=0.0166] \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.16batch/s, val_acc=94, val_loss=0.223]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 64 training loss = 0.0166, training accuracy = 99.61%, val_loss = 0.2235, val_accuracy = 93.98% & LR = 0.0001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: : 100%|| 391/391 [03:53<00:00,  1.67batch/s, accuracy=99.6, loss=0.0164] \n",
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.20batch/s, val_acc=93.8, val_loss=0.225] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 65 training loss = 0.0164, training accuracy = 99.57%, val_loss = 0.2255, val_accuracy = 93.85% & LR = 0.0001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_loss, train_acc = train_model_progress(model, train_loader)\n",
    "    val_loss, val_acc = test_model_progress(model, test_loader)\n",
    "    \n",
    "    print(f\"\\nepoch: {epoch + 1} training loss = {train_loss:.4f}, \"\n",
    "          f\"training accuracy = {train_acc:.2f}%, val_loss = {val_loss:.4f}\"\n",
    "          f\", val_accuracy = {val_acc:.2f}% & \"\n",
    "          f\"LR = {optimizer.param_groups[0]['lr']:.4f}\\n\")\n",
    "    \n",
    "    training_history_lr_scheduler[epoch + 1] = {\n",
    "        'loss': train_loss, 'acc': train_acc,\n",
    "        'val_loss': val_loss, 'val_acc': val_acc,\n",
    "        'lr': optimizer.param_groups[0]['lr']\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Save best weights achieved until now-\n",
    "    if (val_loss < best_val_loss):    \n",
    "        # update 'best_val_loss' variable to lowest loss encountered so far-\n",
    "        best_val_loss = val_loss\n",
    "\n",
    "        print(f\"Saving model with lowest val_loss = {val_loss:.4f}\\n\")\n",
    "        \n",
    "        # Save trained model with 'best' validation accuracy-\n",
    "        torch.save(model.state_dict(), \"ResNet50_lr_scheduler_best_model.pth\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model from last training epoch-\n",
    "torch.save(model.state_dict(), \"ResNet50_lr_scheduler_last_epoch_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new ResNet-50 model-\n",
    "best_model = ResNet50(img_channels = 3, num_channels = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load randomly initialised weights-\n",
    "best_model.load_state_dict(torch.load('ResNet50_lr_scheduler_last_epoch_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (layer1): Sequential(\n",
       "    (0): ResNet_block(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (identity_downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResNet_block(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): ResNet_block(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): ResNet_block(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (identity_downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResNet_block(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): ResNet_block(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (3): ResNet_block(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): ResNet_block(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (identity_downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResNet_block(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): ResNet_block(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (3): ResNet_block(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (4): ResNet_block(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (5): ResNet_block(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): ResNet_block(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (identity_downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResNet_block(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): ResNet_block(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Place model on GPU (if available)-\n",
    "best_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: : 100%|| 79/79 [00:15<00:00,  5.02batch/s, val_acc=93.8, val_loss=0.225] \n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = test_model_progress(best_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet-50 'best' (LR scheduler) model metrics: val_loss = 0.2255 & val_acc = 93.85%\n"
     ]
    }
   ],
   "source": [
    "print(f\"ResNet-50 'best' (LR scheduler) model metrics: val_loss = {val_loss:.4f} & val_acc = {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "del best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "\n",
    "For this particular experiment, it seems that using ```val_loss``` as the metric to save the _best_ model is not the optimum choice.\n",
    "\n",
    "_Highest validation accuracy_ achieved = 93.85%.\n",
    "\n",
    "Also, there seems to be _overfitting_ happening. _Dropout_ needs to be employed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check-\n",
    "training_history_lr_scheduler.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'acc', 'val_loss', 'val_acc', 'lr'])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_history_lr_scheduler[12].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training metrics as Python3 history for later analysis-\n",
    "with open(\"ResNet50_training_history_lr_scheduler.pkl\", \"wb\") as file:\n",
    "    pickle.dump(training_history_lr_scheduler, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ResNet50_training_history_lr_scheduler.pkl\", \"rb\") as file:\n",
    "    training_history_lr_scheduler = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_history_lr_scheduler.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'acc', 'val_loss', 'val_acc', 'lr'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_history_lr_scheduler[12].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(83.492), array(79.2, dtype=float32))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_history_lr_scheduler[12]['acc'], training_history_lr_scheduler[12]['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (9, 7))\n",
    "plt.plot(list(training_history_lr_scheduler.keys()), [training_history_lr_scheduler[k]['acc'] for k in training_history_lr_scheduler.keys()], label = 'training acc')\n",
    "plt.plot(list(training_history_lr_scheduler.keys()), [training_history_lr_scheduler[k]['val_acc'] for k in training_history_lr_scheduler.keys()], label = 'val acc')\n",
    "plt.title(\"ResNet-50: Training Accuracy\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy (%)\")\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (9, 7))\n",
    "plt.plot(list(training_history_lr_scheduler.keys()), [training_history_lr_scheduler[k]['loss'] for k in training_history_lr_scheduler.keys()], label = 'training loss')\n",
    "plt.plot(list(training_history_lr_scheduler.keys()), [training_history_lr_scheduler[k]['val_loss'] for k in training_history_lr_scheduler.keys()], label = 'val loss')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend(loc = 'best')\n",
    "plt.title(\"ResNet-50: Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi8AAAG5CAYAAABGA9SHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAz8ElEQVR4nO3de3RdZ33n/8/33CxfJMtOHMeWHDuB3BzbCokbUiiUFtpfklLCzJQWCqVkVifDDEwL01vKDEPbNcyPWb9OV2GVIWRooEwzw/THrSnjEmhLKWUKxAnRsR0nYEwSbdmOncs5knyTjs53/thb8oki2ZKtrX32c96vtbyss88+R19tW9bHz/N9nm3uLgAAgLwoZF0AAADAQhBeAABArhBeAABArhBeAABArhBeAABArhBeAABArhBeAOAszOxVZvZ41nUAOIPwArQBM3vCzE6a2ZiZHTGzT5nZqgt8z3eYmZvZb844HpnZa+bx+i3J60sLqH3MzL4y4/lfNLMnzey4mX3RzNYu4Gt4wsxeN9/z0+Du33D3q9N4bzN7jZk1k+s2amaPm9kdC3j935nZr6RRG9DOCC9A+/hZd18l6XpJL5P0O4vwns9J+m0z61mE9zqbn3X3Vcmvn546aGbXSfq4pF+StF7SCUn/NeVaFsTMihmXcCj5c++R9F5J/83MUglLQCgIL0Cbcfcjkh5QHGIkSWZ2s5n9HzOrmdlg68hJMsJyMPmf+w/N7K0tb7df0j8q/qH4ImZWMLO7zOwHZvasmf15y8jI3ye/15KRgR89jy/nrZL+0t3/3t3HJL1f0j81s+7zeK/51i0z+/+TEay6mf19EqKmnvuUmX3MzHaZ2XFJP5GM8PyGmVWT1/wvM+tKzn+NmUUtr5/z3OT53zKzw2Z2yMx+JRm9eum5viaP7VIcOHck77XGzL5kZsfM7Pnk4/7kuQ9KepWkP07+fP44OX6NmX3VzJ5LRnJ+/kKuNdCOCC9Am0l+ON0q6UDyuE/S/5b0HyWtlfQbkj5nZuvMbKWkj0i61d27Jb1C0iMz3vL9kt47x3TNr0p6o6Qfl7RR0vOSPpo89+rk995kROUfz1L2fckP2K+Y2UDL8eskDU49cPcfSBqXdFXytd1lZl86y/vO5Wx1S9JfSbpS0iWSHpZ034zX/6KkD0rqlvQPybGfl3SLpMsVh4d3nOXzz3qumd0i6d9Kep2klyb1zUsSyN4g6WIlf/aK/43+pKTNki6TdFLSH0uSu/87Sd+Q9O7kz+fdyd+Hr0r6H8nX/hZJ/7U1vAEhILwA7eOLZjYqaUjSUUkfSI6/TdIud9/l7k13/6qk3ZJuS55vStpmZsvd/bC772t9U3d/RNJXJP32LJ/zX0r6d+4euftpSb8r6efO1ecyw1slbVH8A/Zrkh4ws97kuVWS6jPOrysODXL3D7n76xfwueZVt7vf6+6jLc8NmNnqltf/hbt/M7mep5JjH3H3Q+7+nKS/VMvI1yzmOvfnJX3S3fe5+wlJvzePr2WjmdUUB5MvSPq37v7d5Ot41t0/5+4n3H1UceA6WyB6vaQn3P2T7t5w94clfU7Sz82jDiA3CC9A+3hjMnryGknXKP4fuBSHgjclU0a15Afdj0na4O7HJf2CpHdKOmxm/9vMrpnlvf+DpH9lZpfOOL5Z0hda3ne/pEnF/SkvYmb77Exj7qskKQkBJ5MfsP+vpJri6QxJGlPcy9GqR9LoPK7H2cxZt5kVzexDyZTSiKQnktdc3PL6oVne80jLxycUB6+5zHXuxhnvPf2xmV3Wcu3GWs455O69iq/LRyT9ZMtrVpjZxy1ueB5RPJXXe5Y+nc2SXj7j78pbJc38cwdyjfACtBl3/7qkT0n6g+TQkKT/7u69Lb9WuvuHkvMfcPefkrRB0mOS/tss7/mYpM9Let+Mp4YUTzm1vneXuw9LetEt5939upbG3G/M9SVIsuTjfZKmp5HM7ApJyyR9bx6X4mzOVvcvSrpd8dTNasWjQmqpaarGNByW1N/yeNP0J3R/quXavSgYJaNEvy1pu5m9MTn865KulvRyd+/Rmam8qa9l5tcxJOnrM67LKnf/Vxf8lQFthPACtKc/kvRTZna9pD+T9LNm9v8kowpdSRNpv5mtN7M3JL0OpxWPdEzO8Z6/J+kOSb0tx+6W9EEz2yxJSR/N7clzxxRPSV0xV5HJaMIrzayS1PWbikc4vpmccl9S+6uSGn9f0ueTKZD5KifvPfWrdI66u5Nr8aykFZL+0wI+14X6c0l3mNm1ZrZC8YjXvLn7uKT/0vK6bsXTSbWkZ+kDM17ytF745/MlSVeZ2S+ZWTn59SNmdu35fDFAuyK8AG3I3Y9J+rSk97v7kOKRhPcpDhRDkn5T8fdvQfH/zg8pXqXy45L+9Rzv+UNJ/13SypbDH5Z0v6SvJP0235L08uT8E4p7LL6ZTEHcPMvbdkv6mOKG2WHFTay3uvuzyXvsUzyldZ/iPp7u1vrM7H1m9lfnuBy7FP8An/r1u2erW/F1ezKp59HkuSXh7n+leOrna4qbbqeanE8v4G3ulXSZmf2s4hC7XNIzir+OL88498OKe32eN7OPJKHwpyW9WfHfiSOS/rPi0S4gGOae1ugpAHS2ZMRjr6Rl7t7Iuh4gFIy8AMAiMrN/kkyjrVE86vGXBBdgcRFeAGBx/UvF03s/UNx/RLMssMiYNgIAALnCyAsAAMiVheyi2fYuvvhi37JlS9ZlAACARfDQQw894+7rZh4PKrxs2bJFu3fvzroMAACwCMzsydmOM20EAAByhfACAAByhfACAAByhfACAAByhfACAAByhfACAAByhfACAAByhfACAAByhfACAAByhfACAAByhfACAAByhfACAAByJdXwYma3mNnjZnbAzO6a5flrzOwfzey0mf3GQl4LAAA6U2rhxcyKkj4q6VZJWyW9xcy2zjjtOUm/KukPzuO1AACgA5VSfO+bJB1w94OSZGafkXS7pEenTnD3o5KOmtnPLPS1WDrHTzc0drqhyaZrsulqeuvvWVeHVpVSQVsuWiEzy7oUAEhNmuGlT9JQy+NI0suX4LVYREfqp/Tq/+9rGm+QUvLik+/4Ef3ENZdkXQYApCbN8DLbf/18sV9rZndKulOSLrvssnm+PeZr95PPabzR1Htfd5XW9yxToWAqmqlYsOmP+U9+e3j2+Lje/8W9embsdNalAECq0gwvkaRNLY/7JR1a7Ne6+z2S7pGknTt3zjccYZ6qUV2VUkH/+ideonKRxWnt7OmRU3r/F/dqYpJvAwBhS/On0YOSrjSzy82sIunNku5fgtdiEQ0O1bR1Qw/BJQem/owmaEQCELjURl7cvWFm75b0gKSipHvdfZ+ZvTN5/m4zu1TSbkk9kppm9h5JW919ZLbXplUrZtdsuvYO1/XPbuzPuhTMQ7kYz98RXgCELs1pI7n7Lkm7Zhy7u+XjI4qnhOb1Wiytg8+M6fj4pHb092ZdCuZhauRlnPACIHDMBWBOg0N1SdJA/+qMK8F8TIWXBj0vAAJHeMGcqlFNKypFXbFuVdalYB6KhXgVGNNGAEJHeMGcqsN1betbrWKBtdB5USoY00YAgkd4wawmJpt69NAIU0Y5UykWNNFg2ghA2AgvmNXjR0Z1utGkWTdnyqUC00YAgkd4wayqUdysu4ORl1wpF+l5ARA+wgtmtWe4pt4VZV22dkXWpWABysUCPS8Agkd4wawGh+ra3reauxPnTKVYYKk0gOARXvAipyYm9fjToxqg3yV3ykV6XgCEj/CCF9l3aESTTdd2+l1yp0TPC4AOQHjBi+yJapLEyEsOxT0vTBsBCBvhBS9Sjeq6pHuZLl3dlXUpWKB4nxdGXgCEjfCCFxmMauzvklPlEtNGAMJHeMELjJ6a0MFnjrO/S07RsAugExBe8AJ7h0fkzuZ0eRWHF3peAISN8IIXqCbNukwb5VOFkRcAHYDwgheoRnX1r1mutSsrWZeC88DtAQB0AsILXmAwqrFEOsdKTBsB6ACEF0x77vi4oudP0u+SY9zbCEAnILxgGv0u+Vdh2ghAByC8YFo1qstM2tbXk3UpOE9lNqkD0AEIL5hWjWq64uKV6u4qZ10KzlO5VNBEk54XAGEjvECS5O4ajOo06+bc1CZ17gQYAOEivECS9PTIaR0bPU2zbs5ViiZ3aZLRFwABI7xAUrxEWpK2M/KSa6Vi/C3NcmkAISO8QFLc71IqmK7bSLNunpWT8MJyaQAhI7xAUrzS6Kr13eoqF7MuBRegUjRJYrk0gKARXiB3VzWqa2AT/S55V56eNiK8AAgX4QV66rkTqp+c0Pa+3qxLwQWaCi8Nel4ABIzwAg1GdUlipVEAyiV6XgCEj/ACVYdqqpQKuvrS7qxLwQWi5wVAJyC8QNXhurZu6JmeckB+lQpJz0uDaSMA4eKnVYebbLr2Dtc1wJRREJg2AtAJCC8d7gfHxnRifJI7SQeizLQRgA5AeOlw1aRZl2XSYaiwVBpAByC8dLhqVNPKSlGXX7wq61KwCFgqDaATEF463GBU17a+1SoWLOtSsAi4PQCATkB46WDjjab2HxrRwKberEvBIqmU6HkBED7CSwf73tOjGp9ssjldQKaXShNeAASM8NLBBqOaJGkHtwUIxtRSafZ5ARAywksHqw7VtWZFWZvWLs+6FCySqaXS9LwACBnhpYMNRjVt7++VGc26oWCpNIBOQHjpUCfHJ/X9o2PsrBuYMuEFQAcgvHSoRw/XNdl0be8jvITkTHih5wVAuAgvHWpwaGpn3d5sC8Gi4vYAADoB4aVDVaOa1vcs0/qerqxLwSIyM5UKRngBEDTCS4eqDte5GWOgysUC00YAgkZ46UAjpyZ08Nhx7aDfJUjlomm8wcgLgHARXjrQ3uRO0jvodwlSpVRg2ghA0AgvHag6nIQXRl6CFE8bEV4AhIvw0oGqUU2b1i7XmpWVrEtBCsrFghr0vAAIGOGlAw0O0awbsnLRuD0AgKARXjrMs2OnNVw7yc66AWPaCEDoCC8dZrrfhZGXYLFUGkDoCC8dpjpUl5m0jWbdYJWLbFIHIGyElw5TjWp6ybpVWrWslHUpSEm5WGCfFwBBI7x0EHfXYFTXDvpdgsY+LwBCR3jpIEdGTumZsdMaoN8laOViQY0mPS8AwkV46SBTd5LezshL0Lg9AIDQEV46SDWqqVQwbd3Qk3UpSFGJpdIAAkd46SB7huu6+tJudZWLWZeCFFVYKg0gcKmGFzO7xcweN7MDZnbXLM+bmX0keb5qZje0PPdeM9tnZnvN7H+aWVeatYbO3VWN2Fm3E7BUGkDoUgsvZlaU9FFJt0raKuktZrZ1xmm3Sroy+XWnpI8lr+2T9KuSdrr7NklFSW9Oq9ZO8OSzJ1Q/OcFKow7ADrsAQpfmyMtNkg64+0F3H5f0GUm3zzjndkmf9ti3JPWa2YbkuZKk5WZWkrRC0qEUaw3eYFSTJMJLB2CfFwChSzO89EkaankcJcfOeY67D0v6A0lPSTosqe7uX5ntk5jZnWa228x2Hzt2bNGKD82eqK5lpYKuWt+ddSlIWaXEUmkAYUszvNgsx2b+izrrOWa2RvGozOWSNkpaaWZvm+2TuPs97r7T3XeuW7fuggoOWTWq67qNPSoX6dEOHT0vAEKX5k+ySNKmlsf9evHUz1znvE7SD939mLtPSPq8pFekWGvQJpuuvYdo1u0UUzdmdGf0BUCY0gwvD0q60swuN7OK4obb+2ecc7+ktyerjm5WPD10WPF00c1mtsLMTNJrJe1PsdagHTg6phPjk/S7dIip0TWWSwMIVWp353P3hpm9W9IDilcL3evu+8zsncnzd0vaJek2SQcknZB0R/Lct83ss5IeltSQ9F1J96RVa+iq0826vZnWgaVRLsazsROTTVVKTBMCCE+qtxZ2912KA0rrsbtbPnZJ75rjtR+Q9IE06+sU1aiuVctKuuLilVmXgiVwZuSFvhcAYeK/ZR2gGtW0ra9HhcJs/dEIzVR4GSe8AAgU4SVw442m9h8e5U7SHaSShJcGPS8AAkV4CdzjR0Y1Ptmk36WDlEtnel4AIESEl8Cxs27noecFQOgIL4GrRjWtWVFW/5rlWZeCJVIqJD0vDaaNAISJ8BK4qTtJx9vloBNUmDYCEDjCS8BOjk/q+0fHNMCUUUdh2ghA6AgvAdt3qK7Jpms7zbodhaXSAEJHeAnYYFSXJEZeOkyZpdIAAkd4CdieqKZLe7p0SU9X1qVgCVWYNgIQOMJLwKpRXdsZdek47PMCIHSEl0DVT07o4DPHmTLqQNNLpZk2AhAowkug9g7H/S7srNt5pqeNGoy8AAgT4SVQ1WgqvDDy0mmYNgIQOsJLoKpRTZetXaHeFZWsS8ESY58XAKEjvAQq3lmXUZdOdCa80PMCIEyElwA9M3Zaw7WTGqDfpSOxVBpA6AgvAdpDv0tHKxfpeQEQNsJLgAajmsyk6/oIL52oWIjDC0ulAYSK8BKgalTXS9et0qplpaxLQQbMTJVigZEXAMEivATG3ZNm3d6sS0GGykVjnxcAwSK8BOZw/ZSeGTutgU1MGXWycomRFwDhIrwEphrVJEnb6XfpaOVigZ4XAMEivARmMKqrVDBdu6En61KQoUqxoAYjLwACRXgJzJ6orms2dKurXMy6FGSoXDSmjQAEi/ASkLhZt6btfb1Zl4KMlYoFdtgFECzCS0CeePaERk41NMDmdB0v7nlh5AVAmAgvAZlq1mWZNCpMGwEIGOElINWormWlgq5avyrrUpCxMpvUAQgY4SUg1aim6zb2qFTkj7XTlYsFTTToeQEQJn7KBaIx2dTe4RGmjCAp2aSuycgLgDARXgJx4NiYTk5MsrMuJNHzAiBshJdAVKO6JJp1ESsVmDYCEC7CSyCqUU3dy0q6/KKVWZeCNsC9jQCEjPASiGpU17a+1SoULOtS0AbKRWOfFwDBIrwE4HRjUvsPj2gH/S5IVFgqDSBghJcAPH5kVBOTrgH6XZAoc3sAAAEjvARgMGnW3d7HyAtibFIHIGSElwBUh2pau7Ki/jXLsy4FbaJcYqk0gHARXgJQjera0b9aZjTrIlZh2ghAwAgvOXdivKHvHx1lfxe8QKlQ0GTTNdkkwAAID+El5/YdGlHTpR30u6BFuRSPwjF1BCBEhJecGxyqSRLLpPECleTmnIQXACEivORcNaprw+ouXdLdlXUpaCPl6fDCtBGA8BBecm7PcNysC7SaCi8NRl4ABIjwkmP1kxP64TPHadbFi5SLcc8LtwgAECLCS47tmb6TNCMveKFKiWkjAOEivOTYYFSTJO3o6820DrSfUoGGXQDhIrzk2J6ors0XrdDqFeWsS0GbmZ42ahBeAISH8JJj1ahGvwtmVS4x8gIgXISXnDo2elqH6qc0QL8LZlFhqTSAgBFecqo61e/CyAtmwVJpACEjvORUNaqrYNJ1G3uyLgVtiKXSAEJGeMmpalTTSy9ZpZXLSlmXgjbEDrsAQkZ4ySF3VzWqM2WEOZW5txGAgBFecmi4dlLPHh+nWRdzmpo2IrwACBHhJYemdtbdzsgL5jA18sI+LwBCRHjJocGornLRdO2G7qxLQZvi9gAAQkZ4yaFqVNM1l/ZoWamYdSloU9NLpZuMvAAID+ElZ5pN156ozs0YcVbcHgBAyFINL2Z2i5k9bmYHzOyuWZ43M/tI8nzVzG5oea7XzD5rZo+Z2X4z+9E0a82LJ549rtHTDcILzoql0gBCllp4MbOipI9KulXSVklvMbOtM067VdKVya87JX2s5bkPS/qyu18jaUDS/rRqzZNq0qzLMmmcDUulAYQszZGXmyQdcPeD7j4u6TOSbp9xzu2SPu2xb0nqNbMNZtYj6dWS/kSS3H3c3Wsp1pobg1FNXeWCrrxkVdaloI0VC6aCEV4AhCnN8NInaajlcZQcm885V0g6JumTZvZdM/uEma2c7ZOY2Z1mttvMdh87dmzxqm9Te6K6rtu4WqUi7Uo4u3KxwO0BAAQpzZ+ANsuxmRPwc51TknSDpI+5+8skHZf0op4ZSXL3e9x9p7vvXLdu3YXU2/Yak03tPUSzLuanUixookHPC4DwpBleIkmbWh73Szo0z3MiSZG7fzs5/lnFYaajff/omE5NNDVAvwvmoVwqMG0EIEhphpcHJV1pZpebWUXSmyXdP+Oc+yW9PVl1dLOkursfdvcjkobM7OrkvNdKejTFWnOhGtUkiZEXzEu5aOzzAiBIqd2S2N0bZvZuSQ9IKkq61933mdk7k+fvlrRL0m2SDkg6IemOlrf4N5LuS4LPwRnPdaRqVFd3V0lbLpq1/Qd4gXKxoHGmjQAEKLXwIknuvktxQGk9dnfLxy7pXXO89hFJO9OsL2+qUV3b+1arUJitVQh4oXKRaSMAYWLJSk6cbkzqsSMj7O+CeSsXjfACIEiEl5zYf3hUE5OuAfpdME+MvAAIFeElJ/YkzbrbCS+Yp3ifF3peAISH8JITg1FdF62sqK93edalICfifV4YeQEQHsJLTlSjmnb0r5YZzbqYn3KJpdIAwkR4yYHjpxs6cHSMZl0sCNNGAEJFeMmBfYdG1HQ2p8PClApMGwEIE+ElB87srNubaR3Il0qJpdIAwkR4yYHBqK6Nq7u0rntZ1qUgR1gqDSBUhJcc2BPVGHXBgsXhhZ4XAOEhvLS5+okJPfHsCfZ3wYLFDbuMvAAID+GlzVWHa5KkAUZesECVoqlBeAEQIMJLm6tGdUnS9j5GXrAwTBsBCBXhpc1Vo5q2XLRCq1eUsy4FOVNi2ghAoAgvba4a1WnWxXmpJHeVdmf0BUBYCC9t7OjoKR2un2JzOpyXcrEgd2mySXgBEBbCSxvbk/S7DGzqzbYQ5FK5FH970/cCIDSElzY2GNVVMOm6jT1Zl4IcKhfjb2/6XgCEhvDSxqpRTVde0q0VlVLWpSCHKsX4DuQslwYQGsJLm3L3pFmXfhecn6mRF6aNAISG8NKmhmsn9dzxccILztuZ8MLIC4CwEF7a1NTmdCyTxvkqJdNG9LwACA3hpU0NRjWVi6ZrNnRnXQpyqsLIC4BAEV7a1J6orms39GhZqZh1Kcip6WmjBj0vAMJCeGlDzaZrT1Tnfka4IFP7vDBtBCA0hJc29MNnj2v0dIM7SeOClFkqDSBQhJc2VI1qkqQdmxh5wfmrsFQaQKAIL22oGtW1vFzUS9etyroU5BhLpQGE6pzhxcwKZrZ3KYpBrBrVdd3GHpWKZEucP5ZKAwjVOX86untT0qCZXbYE9XS8xmRT+w7V2d8FF4yl0gBCNd+b5myQtM/MviPp+NRBd39DKlV1sO89PaZTE00N0O+CC8S0EYBQzTe8/F6qVWDanuGaJLFMGhdsaqk0+7wACM28wou7fz3tQhAbjOrq7ippy0Ursy4FOTe1VHqiycgLgLCcNbyY2aik2f7bZpLc3XtSqaqDVaOadvSvVqFgWZeCnJvueWkQXgCE5azhxd25sc4SOjUxqccOj+pfvPqKrEtBAMrs8wIgUKzFbSOPHRlVo+naQb8LFgFLpQGEivDSRs7srNubaR0IQ7nAaiMAYSK8tJHBobouXlXRxtVdWZeCABQKplLBCC8AgkN4aSNxs26vzGjWxeIoFwv0vAAIDuGlTRw/3dCBY2Ps74JFVS4y8gIgPISXNrF3uC53sbMuFlWlVCC8AAgO4aVNVKO6JGl7X2+2hSAo5WKBHXYBBIfw0iYGo5o2ru7Suu5lWZeCgJSYNgIQIMJLm9gzzJ2ksfjKxQL7vAAIDuGlDdROjOvJZ09oB/0uWGSVIj0vAMJDeGkDU/0uA4y8YJGxVBpAiAgvbWBqZ91tLJPGImOpNIAQEV7aQDWq6/KLV2r18nLWpSAwZaaNAASI8NIGqlFdO/oZdcHii/d5YdoIQFgILxk7OnJKR0ZOsdIIqeDeRgBCRHjJ2GDSrMvIC9JQLhY03iC8AAgL4SVj1aimgknXbezJuhQEqMztAQAEiPCSsWpU11Xru7WiUsq6FASowlJpAAEivGTI3VWNakwZITUslQYQIsJLhqLnT+r5ExPaTrMuUsImdQBCRHjJ0GCyOd0AIy9ICfu8AAgR4SVDe6K6KsWCrr60O+tSECimjQCEiPCSocGopms2dGtZqZh1KQgUIy8AQkR4yUiz6do7PEKzLlI11fPiTt8LgHAQXjJy8JkxjZ1usLMuUlUpxd/iNO0CCEmq4cXMbjGzx83sgJndNcvzZmYfSZ6vmtkNM54vmtl3zexLadaZhWqys+4A4QUpKhdNkpg6AhCU1MKLmRUlfVTSrZK2SnqLmW2dcdqtkq5Mft0p6WMznv81SfvTqjFL1aiu5eWiXrJuZdalIGDlYvwt3mDkBUBA0hx5uUnSAXc/6O7jkj4j6fYZ59wu6dMe+5akXjPbIElm1i/pZyR9IsUaMzMY1bStr0elIjN3SM9UeBln5AVAQNL8ydknaajlcZQcm+85fyTptySd9V9dM7vTzHab2e5jx45dUMFLZWKyqUcPjdDvgtRVilM9L4QXAOFIM7zYLMdmjl3Peo6ZvV7SUXd/6FyfxN3vcfed7r5z3bp151Pnkvve06M63Wiy0gipK9HzAiBAaYaXSNKmlsf9kg7N85xXSnqDmT2heLrpJ83sz9IrdWlNNesy8oK0lRl5ARCgNMPLg5KuNLPLzawi6c2S7p9xzv2S3p6sOrpZUt3dD7v777h7v7tvSV73t+7+thRrXVLVqKaerpK2XLQi61IQuOmelwYNuwDCUUrrjd29YWbvlvSApKKke919n5m9M3n+bkm7JN0m6YCkE5LuSKuedlKN6trR3yuz2WbNgMVTKTFtBCA8qYUXSXL3XYoDSuuxu1s+dknvOsd7/J2kv0uhvEycmpjU40dGdeerr8i6FHSA6aXSTcILgHCwTneJPXp4RI2m06yLJcG0EYAQEV6WWHWoJolmXSwNGnYBhIjwssSqw3VdvGqZNqzuyroUdABuDwAgRISXJVaN6hroX02zLpYEIy8AQkR4WUJjpxv6wbExbaffBUvkzO0B6HkBEA7CyxLaO1yXO3eSxtKZvj1Ag5EXAOEgvCyhalSTJFYaYcmUk31eWCoNICSElyU0GNXV17tcF61alnUp6BBMGwEIEeFlCVWjGqMuWFJlpo0ABIjwskSePz6uoedOsr8LlhRLpQGEiPCyRKrD8Z2kBxh5wRJiqTSAEBFelsjUzrrbCC9YQqVCPPJCzwuAkBBelkh1uK4rLl6pnq5y1qWgg5iZKsUCIy8AgkJ4WSI06yIr5aKpQXgBEBDCyxJ4euSUnh45TbMuMlEuFTTBtBGAgBBelsBg0u8ysImRFyy9crGgcUZeAASE8LIE9gzXVSyYtm4gvGDplQvGPi8AgkJ4WQKDUV1XXrJKyyvFrEtBB4qnjQgvAMJBeEmZu6sa1bgZIzJTLtLzAiAshJeURc+fVO3EhHbQ74KM0PMCIDSEl5QNTt1Juq830zrQuSoslQYQGMJLyqpRXZViQVdf2p11KehQTBsBCA3hJWWDQzVdu7FHlRKXGtlg2ghAaPiJmqJm07V3uM7NGJGpUtFYbQQgKISXFB18ZkzHxye1vY/wguxwbyMAoSG8pGhwqC5JGtjUm20h6GjlYkETDXpeAISD8JKiPcN1ragU9ZJ1q7IuBR2MTeoAhIbwkqLBqKZtfatVLFjWpaCDlYtGwy6AoBBeUjIx2dSjh0a0g34XZKxSLKjBUmkAASG8pOTxI6M63WhqB/0uyFiZhl0AgSG8pGTPcNKsyzJpZKzEtBGAwBBeUlKNalq9vKzL1q7IuhR0OJZKAwgN4SUlg0N17ehfLTOadZEtbg8AIDSElxScmpjU40+PagdTRmgD5WJBk03XZJMAAyAMhJcUPHp4RJNN147+3qxLAVQuxaN/TB0BCAXhJQXVoZokaYDwgjZQKcbf5g1GXgAEgvCSgmpU17ruZVrfsyzrUgCVk/Ay0WDkBUAYCC8pGIxqGqBZF22iVGTaCEBYCC+LbPTUhA4+c5x+F7SNqZEX9noBEArCyyLbOzwid7HSCG1jqueF5dIAQkF4WWTVqCZJjLygbUz3vDDyAiAQhJdFVo3q6l+zXGtXVrIuBZAU31VaksZp2AUQCMLLIqsO11gijbZSLrFUGkBYCC+L6Lnj4xp67iT9LmgrFaaNAASG8LKIpvpdthNe0EbY5wVAaAgvi6ga1WUmbe8jvKB9TO3zwlJpAKEgvCyialTXFRevVHdXOetSgGkslQYQGsLLIqpGNZZIo+2wVBpAaAgvi+RI/ZSOjp6mWRdtp8ztAQAEhvCySAbZnA5tqsy0EYDAEF4WyZ6ormLBdN3GnqxLAV6gUmLaCEBYCC+LZDCq6ar13eoqF7MuBXgBel4AhIbwsgjcXXuG6xqg3wVtqMTtAQAEhvCyCJ567oRqJybod0FbYqk0gNAQXhZBNapLEiuN0JaYNgIQGsLLIqhGNVVKBV19aXfWpQAvUiyYCkZ4ARAOwssiGIzq2rqhZ/p/uEC7KRcLTBsBCAY/bS/QZNO1l2ZdtLlKscDIC4BgEF4u0MFjYzoxPkmzLtpauUR4ARAOwssFGqRZFzlQKhjhBUAwCC8XqBrVtLJS1BXrVmVdCjCncrGg8QY9LwDCkGp4MbNbzOxxMztgZnfN8ryZ2UeS56tmdkNyfJOZfc3M9pvZPjP7tTTrvBCDUV3b+larWLCsSwHmVGHaCEBAUgsvZlaU9FFJt0raKuktZrZ1xmm3Sroy+XWnpI8lxxuSft3dr5V0s6R3zfLazI03mtp/eIQpI7S9cpFpIwDhSHPk5SZJB9z9oLuPS/qMpNtnnHO7pE977FuSes1sg7sfdveHJcndRyXtl9SXYq3n5XtPj2q80aRZF22PpdIAQpJmeOmTNNTyONKLA8g5zzGzLZJeJunbs30SM7vTzHab2e5jx45daM0LMhjVJEkDhBe0uTJLpQEEJM3wMlsTyMz/+p31HDNbJelzkt7j7iOzfRJ3v8fdd7r7znXr1p13seejOlTXmhVlbVq7fEk/L7BQ7PMCICRphpdI0qaWx/2SDs33HDMrKw4u97n751Os87xVh+va3t8rM5p10d5K9LwACEia4eVBSVea2eVmVpH0Zkn3zzjnfklvT1Yd3Syp7u6HLU4DfyJpv7v/YYo1nreT45P63tOj2tFHsy7aX7lY0Dg9LwACUUrrjd29YWbvlvSApKKke919n5m9M3n+bkm7JN0m6YCkE5LuSF7+Skm/JGmPmT2SHHufu+9Kq96FevRwXZNNZ6URcqFcLGiiwcgLgDCkFl4kKQkbu2Ycu7vlY5f0rlle9w+avR+mbQwOxTvrDmzqzbYQYB4qJaaNAISDHXbP057hutb3LNP6nq6sSwHOidVGAEJCeDlPg1FN2/t6sy4DmBf2eQEQEsLLeRg5NaGDx45rgH4X5AQjLwBCQng5D3un7iRNvwtygtsDAAgJ4eU8VIeT8MIyaeQE00YAQkJ4OQ/VqKZNa5drzcpK1qUA8xLv88LIC4AwEF7Ow+BQnZsxIlcqybRRvDsBAOQb4WWBnh07reHaSZp1kSvlYkHu0mST8AIg/wgvCzTd78LIC3KkXIq/1RuEFwABILwsUHWoLjNpG826yJFyMf5Wp+8FQAgILwtUjWp6ybpVWrUs1TsrAIuqXIzvtsH9jQCEgPCyAO6uwajOzRiRO1MjLyyXBhACwssCHBk5pWfGTmuAfhfkzJnwwsgLgPwjvCzA1J2ktzPygpyZmjai5wVACAgvC1CNaioVTFs39GRdCrAgFUZeAASE8LIAe4bruvrSbnWVi1mXAizI1LRRg54XAAEgvMyTu6sasbMu8mlqnxemjQCEgPAyT08+e0L1kxOsNEIusVQaQEgIL/M0GNUkifCCXGKpNICQEF7maU9U17JSQVet7866FGDBWCoNICSEl3mqRnVt3dgz/UMAyBOWSgMICT+J52Gy6dp7qM7mdMgtlkoDCAnhZR4OHB3TifFJ+l2QWyyVBhASwss8VKebdXszrQM4XyyVBhASwss8VKO6Vi0r6YqLV2ZdCnBeppdKE14ABIDwMg/VqKZtfT0qFCzrUoDzUi4kPS/s8wIgAISXcxhvNLX/8CjNusi1qWkj9nkBEIJS1gW0u8ePjGp8skm/C3Jtatror/c/redOjGdcDaS4ifqOV2zRmpWVrEsBcofwcg6PHRmRxM66yLdKsaBrLu3Wd5+q6btP1bIuB4qbp3u6SvqVV12RdSlA7hBezuFNOzfpx69ap3Xdy7IuBThvZqYvv+fVWZeBFj/2n/9WDz/1fNZlALlEz8s8XNLTJTOadQEsnp2b12j3E8/LnT4kYKEILwCQgRs3r9HR0dOKnj+ZdSlA7hBeACADN25eK0l66EmmjoCFIrwAQAauvrRbq5aVtPvJ57IuBcgdwgsAZKBYML3ssl499GQt61KA3CG8AEBGbty8Ro8fGdHoqYmsSwFyhfACABm5cfMaNV3svQMsEOEFADJy/aZeFYymXWChCC8AkJHurrKuvrSH8AIsEOEFADK0c/Maffep5zXZZLM6YL4ILwCQoZ1b1uj4+OT0fdQAnBvhBQAydMNlayTR9wIsBOEFADLUv2a51vcsI7wAC0B4AYAMmZluTG7SCGB+CC8AkLEbN6/VcO2kjtRPZV0KkAuEFwDI2M7N9L0AC0F4AYCMbd3Yo65ygZs0AvNEeAGAjJWLBQ309+phRl6AeSG8AEAbuHHzGu07NKKT45NZlwK0PcILALSBnVvWqNF0DUa1rEsB2h7hBQDaAJvVAfNHeAGANtC7oqKXXrJKu5+gaRc4F8ILALSJnZvX6OGnampyk0bgrAgvANAmbti8RvWTE/rBsbGsSwHaGuEFANoEm9UB80N4AYA2cfnFK7V2ZUW7CS/AWRFeAKBNmJluuGwNIy/AORBeAKCN7NyyRj985rieHTuddSlA2yplXQAA4Iwbk76Xe75xUFev7864muxcfWm3rtu4Ousy0KYILwDQRrb3rVZPV0kf//rBrEvJ3E2Xr9U/f+Xl+qmt61UsWNbloI2Ye3r7CZjZLZI+LKko6RPu/qEZz1vy/G2STkh6h7s/PJ/Xzmbnzp2+e/fuxf0iAGCJ1U9OqHZiPOsyMjPZdP3tY0f1yW8+oeHaSfWvWa53vGKLfv5HNqmnq5x1eVhCZvaQu+980fG0wouZFSV9T9JPSYokPSjpLe7+aMs5t0n6N4rDy8slfdjdXz6f186G8AIA4WhMNvXX+5/Wvf/whL7zxHNaWSnqTTs3MRLTZnpXlHXNpT2pvPdc4SXNaaObJB1w94NJAZ+RdLuk1gByu6RPe5ygvmVmvWa2QdKWebwWABCwUrGgW7Zt0C3bNmhPVNcnv/lD3fftJ/Wp//NE1qWhxWuvuUR/8o4fWdLPmWZ46ZM01PI4Ujy6cq5z+ub5WkmSmd0p6U5Juuyyyy6sYgBAW9rev1p/+AvX63duu1bfPzqadTlosWZFZck/Z5rhZbYxvZlzVHOdM5/Xxgfd75F0jxRPGy2kQABAvqzrXqZ13cuyLgMZSzO8RJI2tTzul3RonudU5vFaAADQgdLcpO5BSVea2eVmVpH0Zkn3zzjnfklvt9jNkurufnierwUAAB0otZEXd2+Y2bslPaB4ufO97r7PzN6ZPH+3pF2KVxodULxU+o6zvTatWgEAQH6kus/LUmOpNAAA4ZhrqTT3NgIAALlCeAEAALlCeAEAALlCeAEAALlCeAEAALlCeAEAALlCeAEAALlCeAEAALlCeAEAALlCeAEAALlCeAEAALkS1L2NzOyYpCfP8+UXS3pmEcsJDddnblybuXFtzo7rMzeuzdw66dpsdvd1Mw8GFV4uhJntnu3mT4hxfebGtZkb1+bsuD5z49rMjWvDtBEAAMgZwgsAAMgVwssZ92RdQJvj+syNazM3rs3ZcX3mxrWZW8dfG3peAABArjDyAgAAcoXwAgAAcoXwIsnMbjGzx83sgJndlXU9WTKze83sqJntbTm21sy+ambfT35fk2WNWTGzTWb2NTPbb2b7zOzXkuNcH0lm1mVm3zGzweT6/F5ynOuTMLOimX3XzL6UPObaSDKzJ8xsj5k9Yma7k2Ncm4SZ9ZrZZ83sseTfnx/t9OvT8eHFzIqSPirpVklbJb3FzLZmW1WmPiXplhnH7pL0N+5+paS/SR53ooakX3f3ayXdLOldyd8Vrk/stKSfdPcBSddLusXMbhbXp9WvSdrf8phrc8ZPuPv1LfuXcG3O+LCkL7v7NZIGFP8d6ujr0/HhRdJNkg64+0F3H5f0GUm3Z1xTZtz97yU9N+Pw7ZL+NPn4TyW9cSlrahfuftjdH04+HlX8D0ifuD6SJI+NJQ/LyS8X10eSZGb9kn5G0idaDnNt5sa1kWRmPZJeLelPJMndx929pg6/PoSX+IfPUMvjKDmGM9a7+2Ep/gEu6ZKM68mcmW2R9DJJ3xbXZ1oyLfKIpKOSvuruXJ8z/kjSb0lqthzj2sRc0lfM7CEzuzM5xrWJXSHpmKRPJlOOnzCzlerw60N4kWyWY6wfx5zMbJWkz0l6j7uPZF1PO3H3SXe/XlK/pJvMbFvGJbUFM3u9pKPu/lDWtbSpV7r7DYqn799lZq/OuqA2UpJ0g6SPufvLJB1Xh00RzYbwEo+0bGp53C/pUEa1tKunzWyDJCW/H824nsyYWVlxcLnP3T+fHOb6zJAMa/+d4v4pro/0SklvMLMnFE9N/6SZ/Zm4NpIkdz+U/H5U0hcUT+dzbWKRpCgZxZSkzyoOMx19fQgv0oOSrjSzy82sIunNku7PuKZ2c7+kX04+/mVJf5FhLZkxM1M877zf3f+w5SmujyQzW2dmvcnHyyW9TtJj4vrI3X/H3fvdfYvif2P+1t3fJq6NzGylmXVPfSzppyXtFddGkuTuRyQNmdnVyaHXSnpUHX592GFXkpndpng+uijpXnf/YLYVZcfM/qek1yi+5frTkj4g6YuS/lzSZZKekvQmd5/Z1Bs8M/sxSd+QtEdn+hbep7jvhetjtkNx42BR8X+M/tzdf9/MLhLXZ5qZvUbSb7j767k2kpldoXi0RYqnSP6Hu3+Qa3OGmV2vuNG7IumgpDuUfI+pQ68P4QUAAOQK00YAACBXCC8AACBXCC8AACBXCC8AACBXCC8AACBXCC8AgmBmr5m6WzOAsBFeAABArhBeACwpM3ubmX3HzB4xs48nN3McM7P/YmYPm9nfmNm65NzrzexbZlY1sy+Y2Zrk+EvN7K/NbDB5zUuSt19lZp81s8fM7L5kV2SZ2YfM7NHkff4goy8dwCIhvABYMmZ2raRfUHwjvuslTUp6q6SVkh5Obs73dcU7O0vSpyX9trvvULyz8dTx+yR91N0HJL1C0uHk+MskvUfSVsV3432lma2V9E8kXZe8z39M82sEkD7CC4Cl9FpJN0p60MweSR5fofh2C/8rOefPJP2Yma2W1OvuX0+O/6mkVyf3welz9y9IkrufcvcTyTnfcffI3ZuSHpG0RdKIpFOSPmFm/1TS1LkAcorwAmApmaQ/dffrk19Xu/vvznLe2e5bYmd57nTLx5OSSu7eUHyX4s9JeqOkLy+sZADthvACYCn9jaSfM7NLJMnM1prZZsX/Fv1ccs4vSvoHd69Let7MXpUc/yVJX3f3EUmRmb0xeY9lZrZirk9oZqskrXb3XYqnlK5f9K8KwJIqZV0AgM7h7o+a2b+X9BUzK0iakPQuScclXWdmD0mqK+6LkaRflnR3Ek6m7qYrxUHm42b2+8l7vOksn7Zb0l+YWZfiUZv3LvKXBWCJcVdpAJkzszF3X5V1HQDygWkjAACQK4y8AACAXGHkBQAA5ArhBQAA5ArhBQAA5ArhBQAA5ArhBQAA5Mr/BV3lun6PTp4YAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 648x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (9, 7))\n",
    "plt.plot(list(training_history_lr_scheduler.keys()), [training_history_lr_scheduler[k]['lr'] for k in training_history_lr_scheduler.keys()])\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"lr\")\n",
    "plt.title(\"ResNet-50: Learning-Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "VGG18_PyTorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
