{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545ab774"
   },
   "source": [
    "# VGG-18: Training from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "c1e700f2"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from keras import regularizers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from tensorflow.keras.layers import AveragePooling2D, Conv2D, MaxPooling2D, ReLU\n",
    "from tensorflow.keras import models, layers, datasets\n",
    "from tensorflow.keras.layers import Dense, Flatten, Reshape, Input, InputLayer\n",
    "from tensorflow.keras.layers import Layer, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "42f757b7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ace688e8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4f73a861",
    "outputId": "63f0eb6b-6054-458f-88f1-89755e0591b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensoFlow version: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"TensoFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95fa4af4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f1995a53",
    "outputId": "75cf94c2-6499-4f44-b5c7-7f9314e34d1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "550c51b5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3bf35735"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_classes = 10\n",
    "# num_epochs = 145\n",
    "num_epochs = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0d7aa014"
   },
   "outputs": [],
   "source": [
    "weight_decay = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e55f74f2",
    "outputId": "595b8813-6985-4bfb-f574-77dae4ca124e"
   },
   "outputs": [],
   "source": [
    "# Data preprocessing and cleaning:\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 32, 32\n",
    "\n",
    "# Load CIFAR-10 dataset-\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Load CIFAR-100 dataset-\n",
    "# (X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar100.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b2478468",
    "outputId": "0a3081ed-c7c7-4fd4-ed7d-66bae982d82c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'input_shape' which will be used = (32, 32, 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if tf.keras.backend.image_data_format() == 'channels_first':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 3, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 3, img_rows, img_cols)\n",
    "    input_shape = (3, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 3)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 3)\n",
    "    input_shape = (img_rows, img_cols, 3)\n",
    "\n",
    "print(\"\\n'input_shape' which will be used = {0}\\n\".format(input_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3df56a26"
   },
   "outputs": [],
   "source": [
    "# Convert datasets to floating point types-\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalize the training and testing datasets-\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8e1e166f"
   },
   "outputs": [],
   "source": [
    "# convert class vectors/target to binary class matrices or one-hot encoded values-\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c774a846",
    "outputId": "9bb0d068-99bf-495a-9f7b-1a9b568bcb54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensions of training and testing sets are:\n",
      "X_train.shape = (50000, 32, 32, 3), y_train.shape = (50000, 10)\n",
      "X_test.shape = (10000, 32, 32, 3), y_test.shape = (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDimensions of training and testing sets are:\")\n",
    "print(\"X_train.shape = {0}, y_train.shape = {1}\".format(X_train.shape, y_train.shape))\n",
    "print(\"X_test.shape = {0}, y_test.shape = {1}\".format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0363635"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44fe89e5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fe2a970"
   },
   "source": [
    "### Prepare CIFAR10 dataset for TensorFlow _GradientTape_ training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "33272085"
   },
   "outputs": [],
   "source": [
    "# Create training and testing datasets-\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "f5e88856"
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(buffer_size = 20000, reshuffle_each_iteration = True).batch(batch_size = batch_size, drop_remainder = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "b59dc84a"
   },
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.batch(batch_size=batch_size, drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7b3e409f"
   },
   "outputs": [],
   "source": [
    "# Choose loss function for training-\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate = 0.01, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "9fe0379c"
   },
   "outputs": [],
   "source": [
    "# Select metrics to measure the error & accuracy of model.\n",
    "# These metrics accumulate the values over epochs and then\n",
    "# print the overall result-\n",
    "train_loss = tf.keras.metrics.Mean(name = 'train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name = 'train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name = 'test_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name = 'test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8fc17a9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d93907b7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48cd0104"
   },
   "source": [
    "### Learning Rate Schedule:\n",
    "\n",
    "Training size = 50,000 and batch size = 64. Number of training iterations/steps in one epoch = 781.25.\n",
    "\n",
    "62,500 iterations has 80 epochs and 93,750 iterations has 120 epochs.\n",
    "\n",
    "\n",
    "The learning rate schedule is as follows-\n",
    "\n",
    "1. For the first 62,5001 training steps, use a learning rate of 0.01\n",
    "\n",
    "1. For the next 31,250 training steps, use a learning rate of 0.001\n",
    "\n",
    "1. For any remaing training steps, use a learning rate of 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1a4899d6"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "boundaries = [62500, 93750]\n",
    "values = [0.01, 0.001, 0.0001]\n",
    "\n",
    "learning_rate_fn = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.0012)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate_fn, momentum=0.9)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8144a3e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32c0c6a3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "0e5ce50c"
   },
   "outputs": [],
   "source": [
    "def vgg18_nn():\n",
    "    '''\n",
    "    Function to define the architecture of a convolutional neural network\n",
    "    model following VGG-19 architecture for CIFAR-10 dataset.\n",
    "    \n",
    "    Vgg-19 architecture-\n",
    "    64, 64, pool                 -- convolutional layers\n",
    "    128, 128, pool               -- convolutional layers\n",
    "    256, 256, 256, 256, max-pool -- convolutional layers\n",
    "    512, 512, 512, 512, max-pool -- convolutional layers\n",
    "    512, 512, 512, 512, avg-pool -- convolutional layers\n",
    "    10                           -- fully connected layers\n",
    "    \n",
    "    Output: Returns designed and compiled convolutional neural network model\n",
    "    '''\n",
    "\n",
    "    # l = tf.keras.layers\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(\n",
    "    Conv2D(\n",
    "            filters = 64, kernel_size = (3, 3),\n",
    "            activation='relu', kernel_initializer = tf.initializers.he_normal(),\n",
    "            strides = (1, 1), padding = 'same', kernel_regularizer = regularizers.l2(weight_decay),\n",
    "            input_shape=(32, 32, 3)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(\n",
    "    Conv2D(\n",
    "            filters = 64, kernel_size = (3, 3),\n",
    "            activation='relu', kernel_initializer = tf.initializers.he_normal(),\n",
    "            strides = (1, 1), padding = 'same',\n",
    "            kernel_regularizer = regularizers.l2(weight_decay)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(\n",
    "    MaxPooling2D(\n",
    "            pool_size = (2, 2),\n",
    "            strides = (2, 2)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    model.add(\n",
    "    Conv2D(\n",
    "            filters = 128, kernel_size = (3, 3),\n",
    "            activation='relu', kernel_initializer = tf.initializers.he_normal(),\n",
    "            strides = (1, 1), padding = 'same',\n",
    "            kernel_regularizer = regularizers.l2(weight_decay)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(\n",
    "    Conv2D(\n",
    "            filters = 128, kernel_size = (3, 3),\n",
    "            activation='relu', kernel_initializer = tf.initializers.he_normal(),\n",
    "            strides = (1, 1), padding = 'same',\n",
    "            kernel_regularizer = regularizers.l2(weight_decay)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(\n",
    "    MaxPooling2D(\n",
    "            pool_size = (2, 2),\n",
    "            strides = (2, 2)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    model.add(\n",
    "    Conv2D(\n",
    "            filters = 256, kernel_size = (3, 3),\n",
    "            activation='relu', kernel_initializer = tf.initializers.he_normal(),\n",
    "            strides = (1, 1), padding = 'same',\n",
    "            kernel_regularizer = regularizers.l2(weight_decay)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(\n",
    "    Conv2D(\n",
    "            filters = 256, kernel_size = (3, 3),\n",
    "            activation='relu', kernel_initializer = tf.initializers.he_normal(),\n",
    "            strides = (1, 1), padding = 'same',\n",
    "            kernel_regularizer = regularizers.l2(weight_decay)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(\n",
    "    Conv2D(\n",
    "            filters = 256, kernel_size = (3, 3),\n",
    "            activation='relu', kernel_initializer = tf.initializers.he_normal(),\n",
    "            strides = (1, 1), padding = 'same',\n",
    "            kernel_regularizer = regularizers.l2(weight_decay)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(\n",
    "    Conv2D(\n",
    "            filters = 256, kernel_size = (3, 3),\n",
    "            activation='relu', kernel_initializer = tf.initializers.he_normal(),\n",
    "            strides = (1, 1), padding = 'same',\n",
    "            kernel_regularizer = regularizers.l2(weight_decay)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(\n",
    "    MaxPooling2D(\n",
    "            pool_size = (2, 2),\n",
    "            strides = (2, 2)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    model.add(\n",
    "    Conv2D(\n",
    "            filters = 512, kernel_size = (3, 3),\n",
    "            activation='relu', kernel_initializer = tf.initializers.he_normal(),\n",
    "            strides = (1, 1), padding = 'same',\n",
    "            kernel_regularizer = regularizers.l2(weight_decay)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(\n",
    "    Conv2D(\n",
    "            filters = 512, kernel_size = (3, 3),\n",
    "            activation='relu', kernel_initializer = tf.initializers.he_normal(),\n",
    "            strides = (1, 1), padding = 'same',\n",
    "            kernel_regularizer = regularizers.l2(weight_decay)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(\n",
    "    Conv2D(\n",
    "            filters = 512, kernel_size = (3, 3),\n",
    "            activation='relu', kernel_initializer = tf.initializers.he_normal(),\n",
    "            strides = (1, 1), padding = 'same',\n",
    "            kernel_regularizer = regularizers.l2(weight_decay)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(\n",
    "    Conv2D(\n",
    "            filters = 512, kernel_size = (3, 3),\n",
    "            activation='relu', kernel_initializer = tf.initializers.he_normal(),\n",
    "            strides = (1, 1), padding = 'same',\n",
    "            kernel_regularizer = regularizers.l2(weight_decay)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(\n",
    "    MaxPooling2D(\n",
    "            pool_size = (2, 2),\n",
    "            strides = (2, 2)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    model.add(\n",
    "    Conv2D(\n",
    "            filters = 512, kernel_size = (3, 3),\n",
    "            activation='relu', kernel_initializer = tf.initializers.he_normal(),\n",
    "            strides = (1, 1), padding = 'same',\n",
    "            kernel_regularizer = regularizers.l2(weight_decay)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(\n",
    "    Conv2D(\n",
    "            filters = 512, kernel_size = (3, 3),\n",
    "            activation='relu', kernel_initializer = tf.initializers.he_normal(),\n",
    "            strides = (1, 1), padding = 'same',\n",
    "            kernel_regularizer = regularizers.l2(weight_decay)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(\n",
    "    Conv2D(\n",
    "            filters = 512, kernel_size = (3, 3),\n",
    "            activation='relu', kernel_initializer = tf.initializers.he_normal(),\n",
    "            strides = (1, 1), padding = 'same',\n",
    "            kernel_regularizer = regularizers.l2(weight_decay)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(\n",
    "    Conv2D(\n",
    "            filters = 512, kernel_size = (3, 3),\n",
    "            activation='relu', kernel_initializer = tf.initializers.he_normal(),\n",
    "            strides = (1, 1), padding = 'same',\n",
    "            kernel_regularizer = regularizers.l2(weight_decay)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    \n",
    "    model.add(\n",
    "        # AveragePooling2D(\n",
    "        MaxPooling2D(\n",
    "            pool_size=(2, 2), strides=(2, 2)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "  \n",
    "    model.add(\n",
    "    Dense(\n",
    "            units = 512, activation='relu',\n",
    "            kernel_initializer = tf.initializers.he_normal(),\n",
    "            kernel_regularizer=regularizers.l2(weight_decay)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.5))\n",
    "    \n",
    "    '''\n",
    "    model.add(\n",
    "    Dense(\n",
    "            units = 256, activation='relu',\n",
    "            kernel_initializer = tf.initializers.he_normal()\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dropout(0.4))\n",
    "    '''\n",
    "    \n",
    "    model.add(\n",
    "    Dense(\n",
    "            units = num_classes, activation='softmax'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGBlock(Layer):\n",
    "    def __init__(self, num_filters = 32, use_four_block = False, weight_decay = 0.0005):\n",
    "        super(VGGBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = Conv2D(\n",
    "            filters = num_filters, kernel_size = (3, 3),\n",
    "            kernel_initializer = tf.initializers.he_normal(),\n",
    "            strides = (1, 1), padding = 'same',\n",
    "            use_bias = False,\n",
    "            kernel_regularizer = regularizers.l2(weight_decay)\n",
    "            )\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.relu1 = Activation(tf.nn.relu)\n",
    "        \n",
    "        self.conv2 = Conv2D(\n",
    "            filters = num_filters, kernel_size = (3, 3),\n",
    "            kernel_initializer = tf.initializers.he_normal(),\n",
    "            strides = (1, 1), padding = 'same',\n",
    "            use_bias = False,\n",
    "            kernel_regularizer = regularizers.l2(weight_decay)\n",
    "            )\n",
    "        self.bn2 = BatchNormalization()\n",
    "        self.relu2 = Activation(tf.nn.relu)\n",
    "        \n",
    "        if use_four_block:\n",
    "            self.conv3 = Conv2D(\n",
    "                filters = num_filters, kernel_size = (3, 3),\n",
    "                kernel_initializer = tf.initializers.he_normal(),\n",
    "                strides = (1, 1), padding = 'same',\n",
    "                use_bias = False,\n",
    "                kernel_regularizer = regularizers.l2(weight_decay)\n",
    "            )\n",
    "            self.bn3 = BatchNormalization()\n",
    "            self.relu3 = Activation(tf.nn.relu)\n",
    "            \n",
    "            self.conv4 = Conv2D(\n",
    "                filters = num_filters, kernel_size = (3, 3),\n",
    "                kernel_initializer = tf.initializers.he_normal(),\n",
    "                strides = (1, 1), padding = 'same',\n",
    "                use_bias = False,\n",
    "                kernel_regularizer = regularizers.l2(weight_decay)\n",
    "            )\n",
    "            self.bn4 = BatchNormalization()\n",
    "            self.relu4 = Activation(tf.nn.relu)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "            \n",
    "        \n",
    "        self.pool = MaxPooling2D(\n",
    "            pool_size = (2, 2),\n",
    "            strides = (2, 2)\n",
    "            )\n",
    "        \n",
    "        # self.pool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.relu1(self.bn1(self.conv1(inputs)))\n",
    "        x = self.relu2(self.bn2(self.conv2(x)))\n",
    "        \n",
    "        # if use_four_block:\n",
    "        if self.conv3:\n",
    "            x = self.relu3(self.bn3(self.conv3(x)))\n",
    "            x = self.relu4(self.bn4(self.conv4(x)))\n",
    "        \n",
    "        return self.pool(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19(Model):\n",
    "    def __init__(self):\n",
    "        super(VGG19, self).__init__()\n",
    "        \n",
    "        self.block1 = VGGBlock(num_filters = 64)\n",
    "        self.block2 = VGGBlock(num_filters = 128)\n",
    "        self.block3 = VGGBlock(num_filters = 256, use_four_block = True)\n",
    "        self.block4 = VGGBlock(num_filters = 512, use_four_block = True)\n",
    "        self.block5 = VGGBlock(num_filters = 512, use_four_block = True)\n",
    "        \n",
    "        weight_decay = 0.0005\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        self.dense1 =  Dense(\n",
    "            units = 512, kernel_initializer = tf.initializers.he_normal(),\n",
    "            kernel_regularizer = regularizers.l2(weight_decay)\n",
    "            )\n",
    "        # self.bn_dense = BatchNormalization()\n",
    "        self.relu_dense = Activation(tf.nn.relu)\n",
    "        # self.dropout_dense = Dropout(0.5)\n",
    "        \n",
    "        '''\n",
    "        self.dense2 = Dense(\n",
    "            units = 256, kernel_initializer = tf.initializers.he_normal()\n",
    "            )\n",
    "        self.relu2 = Activation(tf.nn.relu)\n",
    "        # self.dropout2 = Dropout(0.4)\n",
    "        '''\n",
    "\n",
    "        self.op = Dense(\n",
    "            units = num_classes, activation = 'softmax'\n",
    "            )\n",
    "    \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.block1(inputs)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.flatten(x)\n",
    "        # x = self.dropout1(self.relu1(self.dense1(x)))\n",
    "        x = self.relu_dense(self.dense1(x))\n",
    "        # x = self.dropout2(self.relu2(self.dense2(x)))\n",
    "        # x = self.relu2(self.dense2(x))\n",
    "        return self.op(x)\n",
    "    \n",
    "    \n",
    "    def model(self):\n",
    "        '''\n",
    "        Overrides 'model()' call.\n",
    "        Output shape is not well-defined when using sub-classing. As a\n",
    "        workaround, this method is implemeted.\n",
    "        '''\n",
    "        x = Input(shape = (32, 32, 3))\n",
    "        return Model(inputs = [x], outputs = self.call(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "5c27dc2a"
   },
   "outputs": [],
   "source": [
    "# Initialize a CNN model-\n",
    "# orig_model = vgg18_nn()\n",
    "\n",
    "orig_model = VGG19()\n",
    "orig_model.build(input_shape = (None, 32, 32, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "EKp9_swVtclz"
   },
   "outputs": [],
   "source": [
    "# Load random weights of defined CNN-\n",
    "# orig_model.load_weights(\"VGG18_Random_Weights_henormal_weight_decay_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "5b4f63f0"
   },
   "outputs": [],
   "source": [
    "# Save current weights-\n",
    "orig_model.save_weights(\"VGG18_batchnorm_random_weights.h5\", overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWX1t4lNtjTb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "8e25e0a7"
   },
   "outputs": [],
   "source": [
    "# lr_decay = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7bdae4eb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dd1f1659",
    "outputId": "cc01b417-0289-4e2f-ff55-d30de90cb6f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "vgg_block_2 (VGGBlock)       (None, 16, 16, 64)        39104     \n",
      "_________________________________________________________________\n",
      "vgg_block_3 (VGGBlock)       (None, 8, 8, 128)         222208    \n",
      "_________________________________________________________________\n",
      "vgg_block_4 (VGGBlock)       (None, 4, 4, 256)         2068480   \n",
      "_________________________________________________________________\n",
      "vgg_block_5 (VGGBlock)       (None, 2, 2, 512)         8265728   \n",
      "_________________________________________________________________\n",
      "vgg_block_6 (VGGBlock)       (None, 1, 1, 512)         9445376   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 20,308,682\n",
      "Trainable params: 20,297,674\n",
      "Non-trainable params: 11,008\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get CNN model summary-\n",
    "orig_model.model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c009c4df"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7b8b7fb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82101f47"
   },
   "source": [
    "### Image Augmentation using _ImageDataGenerator_:\n",
    "\n",
    "Refer-\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "4a1cb243",
    "outputId": "b134bb51-e26d-4738-b359-7805b5acb336"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor x, y in datagen.flow(X_train, y_train, batch_size=batch_size, shuffle=True):\\n\\tprint(x.shape, y.shape)\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of using 'tf.keras.preprocessing.image import ImageDataGenerator class's - flow(x, y)':\n",
    "datagen = ImageDataGenerator(\n",
    "    # featurewise_center=True,\n",
    "    # featurewise_std_normalization=True,\n",
    "    rotation_range = 15,\n",
    "    width_shift_range = 0.1, # 0.2\n",
    "    height_shift_range = 0.1, # 0.2\n",
    "    horizontal_flip = True\n",
    ")\n",
    "\n",
    "\n",
    "# flow():\n",
    "# Takes data & label arrays, generates batches of augmented data.\n",
    "\n",
    "# datagen.flow(X_train, y_train, batch_size=batch_size, shuffle=True)\n",
    "'''\n",
    "for x, y in datagen.flow(X_train, y_train, batch_size=batch_size, shuffle=True):\n",
    "\tprint(x.shape, y.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "a1f8f18d"
   },
   "outputs": [],
   "source": [
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22a1a607"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "925d5dc9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dd152d91",
    "outputId": "52736182-246c-410c-9f3c-4b49508f0512"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of training weights = 20297674 and non-trainabel weights = 11008\n",
      "\n",
      "Total number of parameters = 20308682\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "# METHOD-1: This also counts biases\n",
    "trainable_wts = np.sum([K.count_params(w) for w in orig_model.trainable_weights])\n",
    "non_trainable_wts = np.sum([K.count_params(w) for w in orig_model.non_trainable_weights])\n",
    "\n",
    "print(\"\\nNumber of training weights = {0} and non-trainabel weights = {1}\\n\".format(\n",
    "    trainable_wts, non_trainable_wts\n",
    "))\n",
    "print(\"Total number of parameters = {0}\\n\".format(trainable_wts + non_trainable_wts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "890dc832"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "316af289",
    "outputId": "a185eedd-c366-4f0a-c8b5-b0f2d3c63089"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer.shape = (3, 3, 3, 64) has 1728 trainable weights\n",
      "layer.shape = (64,) has 64 trainable weights\n",
      "layer.shape = (64,) has 0 trainable weights\n",
      "layer.shape = (3, 3, 64, 64) has 36864 trainable weights\n",
      "layer.shape = (64,) has 64 trainable weights\n",
      "layer.shape = (64,) has 0 trainable weights\n",
      "layer.shape = (3, 3, 64, 128) has 73728 trainable weights\n",
      "layer.shape = (128,) has 128 trainable weights\n",
      "layer.shape = (128,) has 0 trainable weights\n",
      "layer.shape = (3, 3, 128, 128) has 147456 trainable weights\n",
      "layer.shape = (128,) has 128 trainable weights\n",
      "layer.shape = (128,) has 0 trainable weights\n",
      "layer.shape = (3, 3, 128, 256) has 294912 trainable weights\n",
      "layer.shape = (256,) has 256 trainable weights\n",
      "layer.shape = (256,) has 0 trainable weights\n",
      "layer.shape = (3, 3, 256, 256) has 589824 trainable weights\n",
      "layer.shape = (256,) has 256 trainable weights\n",
      "layer.shape = (256,) has 0 trainable weights\n",
      "layer.shape = (3, 3, 256, 256) has 589824 trainable weights\n",
      "layer.shape = (256,) has 256 trainable weights\n",
      "layer.shape = (256,) has 0 trainable weights\n",
      "layer.shape = (3, 3, 256, 256) has 589824 trainable weights\n",
      "layer.shape = (256,) has 256 trainable weights\n",
      "layer.shape = (256,) has 0 trainable weights\n",
      "layer.shape = (3, 3, 256, 512) has 1179648 trainable weights\n",
      "layer.shape = (512,) has 512 trainable weights\n",
      "layer.shape = (512,) has 0 trainable weights\n",
      "layer.shape = (3, 3, 512, 512) has 2359296 trainable weights\n",
      "layer.shape = (512,) has 512 trainable weights\n",
      "layer.shape = (512,) has 0 trainable weights\n",
      "layer.shape = (3, 3, 512, 512) has 2359296 trainable weights\n",
      "layer.shape = (512,) has 512 trainable weights\n",
      "layer.shape = (512,) has 0 trainable weights\n",
      "layer.shape = (3, 3, 512, 512) has 2359296 trainable weights\n",
      "layer.shape = (512,) has 512 trainable weights\n",
      "layer.shape = (512,) has 0 trainable weights\n",
      "layer.shape = (3, 3, 512, 512) has 2359296 trainable weights\n",
      "layer.shape = (512,) has 512 trainable weights\n",
      "layer.shape = (512,) has 0 trainable weights\n",
      "layer.shape = (3, 3, 512, 512) has 2359296 trainable weights\n",
      "layer.shape = (512,) has 512 trainable weights\n",
      "layer.shape = (512,) has 0 trainable weights\n",
      "layer.shape = (3, 3, 512, 512) has 2359296 trainable weights\n",
      "layer.shape = (512,) has 512 trainable weights\n",
      "layer.shape = (512,) has 0 trainable weights\n",
      "layer.shape = (3, 3, 512, 512) has 2359296 trainable weights\n",
      "layer.shape = (512,) has 512 trainable weights\n",
      "layer.shape = (512,) has 0 trainable weights\n",
      "layer.shape = (512, 512) has 262144 trainable weights\n",
      "layer.shape = (512,) has 0 trainable weights\n",
      "layer.shape = (512, 10) has 5120 trainable weights\n",
      "layer.shape = (10,) has 0 trainable weights\n",
      "\n",
      "VGG-18 has a total of 20291648 trainable parameters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tot_trainable_params = 0\n",
    "\n",
    "for layer in orig_model.trainable_weights:\n",
    "    loc_params = tf.math.count_nonzero(layer, axis = None).numpy()\n",
    "    print(\"layer.shape = {0} has {1} trainable weights\".format(layer.shape, loc_params))\n",
    "    tot_trainable_params += loc_params\n",
    "    \n",
    "print(\"\\nVGG-18 has a total of {0} trainable parameters\\n\".format(tot_trainable_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4de0687"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "29e56cc0",
    "outputId": "4befce51-c6fd-488a-ee60-0f37289ede18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "According to tf.keras.backend: Number of training weights = 20297674, non-trainabel weights = 11008 and total number of parameters = 20308682\n",
      "\n",
      "According to tf.math.count_nonzero() method, total number of trainable weights = 20291648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAccording to tf.keras.backend: Number of training weights = {0}, non-trainabel weights = {1} and total number of parameters = {2}\\n\".format(\n",
    "    trainable_wts, non_trainable_wts, (trainable_wts + non_trainable_wts)\n",
    "))\n",
    "\n",
    "print(\"According to tf.math.count_nonzero() method, total number of trainable weights = {0}\\n\".format(tot_trainable_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "889821c7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57854422"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "ab05f13b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# For each layer, for each weight which is 0, leave it, as is.\\n# And for weights which survive the pruning,reinitialize it to ONE (1)-\\nfor wts in mask_model.trainable_weights:\\n    wts.assign(tf.where(tf.equal(wts, 0.), 0., 1.))\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create mask for original, unpruned model-\n",
    "# mask_model = vgg18_nn()\n",
    "mask_model = VGG19()\n",
    "mask_model.build(input_shape = (None, 32, 32, 3))\n",
    "\n",
    "\n",
    "# Assign all masks to one-\n",
    "for wts in mask_model.trainable_weights:\n",
    "    wts.assign(\n",
    "        tf.ones_like(input = wts, dtype = tf.float32\n",
    "                    )\n",
    "    )\n",
    "\n",
    "'''\n",
    "# For each layer, for each weight which is 0, leave it, as is.\n",
    "# And for weights which survive the pruning,reinitialize it to ONE (1)-\n",
    "for wts in mask_model.trainable_weights:\n",
    "    wts.assign(tf.where(tf.equal(wts, 0.), 0., 1.))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6986df1",
    "outputId": "a184c7d3-4c2f-45a1-9ebb-15a4cd8131e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "number of non-zero masks = 20297674\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sanity check-\n",
    "mask_params = 0\n",
    "\n",
    "for layer in mask_model.trainable_weights:\n",
    "    loc_params = tf.math.count_nonzero(layer, axis = None).numpy()\n",
    "    mask_params += loc_params\n",
    "    \n",
    "print(\"\\nnumber of non-zero masks = {0}\\n\".format(mask_params))\n",
    "# VGG-19 has a total of 20292095 trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d2a8ea88",
    "outputId": "aae37faf-90a6-4a9f-8383-cca82d6057bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_params == tot_trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "466b9c8b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9861c1f3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "KHnHHS9JvQ-W"
   },
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    model_sum_params = 0\n",
    "\n",
    "    for layer in model.trainable_weights:\n",
    "        # print(tf.math.count_nonzero(layer, axis = None).numpy())\n",
    "        model_sum_params += tf.math.count_nonzero(layer, axis = None).numpy()\n",
    "    \n",
    "    return model_sum_params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ce2777b",
    "outputId": "b2020682-4e7e-4acf-df50-b4e6d7c18fe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG-18 has 20291648 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# Sanity check-\n",
    "print(f\"VGG-18 has {count_params(orig_model)} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7Np5tjOvk3l"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0924b97d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile defined CNN-\n",
    "orig_model.compile(\n",
    "    loss=tf.keras.losses.categorical_crossentropy,\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate = 0.1, momentum = 0.9),\n",
    "    # optimizer = tf.keras.optimizers.SGD(learning_rate = learning_rate_fn, momentum = 0.9),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate warmup:\n",
    "\n",
    "Using a large learning rate may result in numerical instability especially at the very beginning of training where parameters are randomly initialized. The warmup strategy increases the learning rate from 0 to the initial learning rate linearly during the initial __N__ epochs or __m__ batches.\n",
    "\n",
    "Even though Keras already came with the _LearningRateScheduler_ capable of updating the learning rate for each training epoch, in order to achieve finer updates for each batch, the following code is how you can implement a custom Keras callback to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmUp(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    Applies a warmup schedule on a given learning rate decay schedule.\n",
    "\n",
    "    Args:\n",
    "        initial_learning_rate (:obj:`float`):\n",
    "            The initial learning rate for the schedule after the warmup (so this will be the learning rate at the end\n",
    "            of the warmup).\n",
    "            \n",
    "        decay_schedule_fn (:obj:`Callable`):\n",
    "            The schedule function to apply after the warmup for the rest of training.\n",
    "        \n",
    "        warmup_steps (:obj:`int`):\n",
    "            The number of steps for the warmup part of training.\n",
    "        \n",
    "        power (:obj:`float`, `optional`, defaults to 1):\n",
    "            The power to use for the polynomial warmup (defaults is a linear warmup).\n",
    "        \n",
    "        name (:obj:`str`, `optional`):\n",
    "            Optional name prefix for the returned tensors during the schedule.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        initial_learning_rate: float,\n",
    "        decay_schedule_fn: Callable,\n",
    "        warmup_steps: int,\n",
    "        power: float = 1.0,\n",
    "        name: str = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.power = power\n",
    "        self.decay_schedule_fn = decay_schedule_fn\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, step):\n",
    "        with tf.name_scope(self.name or \"WarmUp\") as name:\n",
    "            # Implements polynomial warmup. i.e., if global_step < warmup_steps, the\n",
    "            # learning rate will be `global_step/num_warmup_steps * init_lr`.\n",
    "            global_step_float = tf.cast(step, tf.float32)\n",
    "            warmup_steps_float = tf.cast(self.warmup_steps, tf.float32)\n",
    "            warmup_percent_done = global_step_float / warmup_steps_float\n",
    "            warmup_learning_rate = self.initial_learning_rate * tf.math.pow(warmup_percent_done, self.power)\n",
    "            return tf.cond(\n",
    "                global_step_float < warmup_steps_float,\n",
    "                lambda: warmup_learning_rate,\n",
    "                lambda: self.decay_schedule_fn(step - self.warmup_steps),\n",
    "                name=name,\n",
    "            )\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"initial_learning_rate\": self.initial_learning_rate,\n",
    "            \"decay_schedule_fn\": self.decay_schedule_fn,\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "            \"power\": self.power,\n",
    "            \"name\": self.name,\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Decay: Learning Rate Schedule\n",
    "\n",
    "For CIFAR-10 dataset, the training dataset has 50K images and batch size = 64. Therefore, number of training steps in one epoch = 781.25 = 781. So, 10 epochs has 7813 training steps/iterations.\n",
    "\n",
    "__After 10 epochs or 7813 training steps,__ the learning rate schedule is as follows-\n",
    "\n",
    "- For the next 21094 training steps (or, 27 epochs), use a learning rate of 0.1\n",
    "\n",
    "- For the next 13282 training steps (or, 17 epochs), use a learning rate of 0.01\n",
    "\n",
    "- For any remaing training steps, use a learning rate of 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundaries = [21093, 34376]\n",
    "values = [0.1, 0.01, 0.001]\n",
    "\n",
    "learning_rate_fn = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_shcedule = WarmUp(initial_learning_rate = 0.1, decay_schedule_fn = learning_rate_fn, warmup_steps = 7813)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate = warmup_shcedule, momentum = 0.9, decay = 0.0, nesterov = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ad54f9b0"
   },
   "source": [
    "### Train model using _tf.GradientTape_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9307dd9c"
   },
   "outputs": [],
   "source": [
    "# User input parameters for Early Stopping in manual implementation-\n",
    "# minimum_delta = 0.001\n",
    "# patience = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "b8ea79e4"
   },
   "outputs": [],
   "source": [
    "best_val_loss = 100\n",
    "# loc_patience = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b6718ce3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "ed8d7e43"
   },
   "outputs": [],
   "source": [
    "# Define 'train_one_step()' and 'test_step()' functions here-\n",
    "@tf.function\n",
    "def train_one_step(model, mask_model, optimizer, x, y):\n",
    "    '''\n",
    "    Function to compute one step of gradient descent optimization\n",
    "    '''\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Make predictions using defined model-\n",
    "        y_pred = model(x)\n",
    "\n",
    "        # Compute loss-\n",
    "        loss = loss_fn(y, y_pred)\n",
    "        \n",
    "    # Compute gradients wrt defined loss and weights and biases-\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    # type(grads)\n",
    "    # list\n",
    "    \n",
    "    # List to hold element-wise multiplication between-\n",
    "    # computed gradient and masks-\n",
    "    grad_mask_mul = []\n",
    "    \n",
    "    # Perform element-wise multiplication between computed gradients and masks-\n",
    "    for grad_layer, mask in zip(grads, mask_model.trainable_weights):\n",
    "        grad_mask_mul.append(tf.math.multiply(grad_layer, mask))\n",
    "    \n",
    "    # Apply computed gradients to model's weights and biases-\n",
    "    optimizer.apply_gradients(zip(grad_mask_mul, model.trainable_variables))\n",
    "\n",
    "    # Compute accuracy-\n",
    "    train_loss(loss)\n",
    "    train_accuracy(y, y_pred)\n",
    "\n",
    "    return None\n",
    "    \n",
    "    \n",
    "@tf.function\n",
    "def test_step(model, optimizer, data, labels):\n",
    "    \"\"\"\n",
    "    Function to test model performance\n",
    "    on testing dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = model(data)\n",
    "    t_loss = loss_fn(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n",
    "\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python3 dict to contain model training metrics-\n",
    "history_lr = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "f2515dfe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bd6b04a0",
    "outputId": "0678acf2-3876-4f8e-c46c-f1e7b7ce6254"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1, Loss: 1.9039, Accuracy: 28.7912, Test Loss: 1.5985, Test Accuracy: 43.349998, LR: 0.009996, Current step value: 781\n",
      "VGG-18: total # of trainable parameters = 20297654\n",
      "Saving model with lowest val_loss = 1.5985\n",
      "\n",
      "\n",
      "Epoch 2, Loss: 1.5104, Accuracy: 44.8564, Test Loss: 1.3506, Test Accuracy: 51.590000, LR: 0.019992, Current step value: 1562\n",
      "VGG-18: total # of trainable parameters = 20297667\n",
      "Saving model with lowest val_loss = 1.3506\n",
      "\n",
      "\n",
      "Epoch 3, Loss: 1.3259, Accuracy: 52.6208, Test Loss: 1.1812, Test Accuracy: 59.670002, LR: 0.029988, Current step value: 2343\n",
      "VGG-18: total # of trainable parameters = 20297673\n",
      "Saving model with lowest val_loss = 1.1812\n",
      "\n",
      "\n",
      "Epoch 4, Loss: 1.2224, Accuracy: 57.3484, Test Loss: 1.0485, Test Accuracy: 64.610001, LR: 0.039985, Current step value: 3124\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "Saving model with lowest val_loss = 1.0485\n",
      "\n",
      "\n",
      "Epoch 5, Loss: 1.1588, Accuracy: 60.1473, Test Loss: 1.1105, Test Accuracy: 63.300003, LR: 0.049981, Current step value: 3905\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 6, Loss: 1.0939, Accuracy: 63.1302, Test Loss: 0.9831, Test Accuracy: 67.769997, LR: 0.059977, Current step value: 4686\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "Saving model with lowest val_loss = 0.9831\n",
      "\n",
      "\n",
      "Epoch 7, Loss: 1.0643, Accuracy: 64.4546, Test Loss: 1.0071, Test Accuracy: 67.189995, LR: 0.069973, Current step value: 5467\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 8, Loss: 1.0444, Accuracy: 65.7730, Test Loss: 1.0000, Test Accuracy: 67.250000, LR: 0.079969, Current step value: 6248\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 9, Loss: 1.0380, Accuracy: 66.1212, Test Loss: 0.9066, Test Accuracy: 71.779999, LR: 0.089965, Current step value: 7029\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "Saving model with lowest val_loss = 0.9066\n",
      "\n",
      "\n",
      "Epoch 10, Loss: 1.0219, Accuracy: 67.0375, Test Loss: 0.8925, Test Accuracy: 73.119995, LR: 0.099962, Current step value: 7810\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "Saving model with lowest val_loss = 0.8925\n",
      "\n",
      "\n",
      "Epoch 11, Loss: 0.9626, Accuracy: 69.0521, Test Loss: 1.0474, Test Accuracy: 63.389999, LR: 0.100000, Current step value: 8591\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 12, Loss: 0.8929, Accuracy: 71.1528, Test Loss: 0.7755, Test Accuracy: 75.169998, LR: 0.100000, Current step value: 9372\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "Saving model with lowest val_loss = 0.7755\n",
      "\n",
      "\n",
      "Epoch 13, Loss: 0.8189, Accuracy: 73.6756, Test Loss: 0.8127, Test Accuracy: 73.750000, LR: 0.100000, Current step value: 10153\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 14, Loss: 0.7825, Accuracy: 74.8339, Test Loss: 0.7123, Test Accuracy: 77.820000, LR: 0.100000, Current step value: 10934\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "Saving model with lowest val_loss = 0.7123\n",
      "\n",
      "\n",
      "Epoch 15, Loss: 0.7314, Accuracy: 76.4205, Test Loss: 0.6646, Test Accuracy: 79.089996, LR: 0.100000, Current step value: 11715\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "Saving model with lowest val_loss = 0.6646\n",
      "\n",
      "\n",
      "Epoch 16, Loss: 0.6974, Accuracy: 77.3408, Test Loss: 0.6436, Test Accuracy: 79.659996, LR: 0.100000, Current step value: 12496\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "Saving model with lowest val_loss = 0.6436\n",
      "\n",
      "\n",
      "Epoch 17, Loss: 0.6670, Accuracy: 78.3831, Test Loss: 0.6550, Test Accuracy: 78.829994, LR: 0.100000, Current step value: 13277\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 18, Loss: 0.6500, Accuracy: 79.1073, Test Loss: 0.6105, Test Accuracy: 80.349998, LR: 0.100000, Current step value: 14058\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "Saving model with lowest val_loss = 0.6105\n",
      "\n",
      "\n",
      "Epoch 19, Loss: 0.6255, Accuracy: 80.0076, Test Loss: 0.5948, Test Accuracy: 80.900002, LR: 0.100000, Current step value: 14839\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "Saving model with lowest val_loss = 0.5948\n",
      "\n",
      "\n",
      "Epoch 20, Loss: 0.5970, Accuracy: 80.7438, Test Loss: 0.5310, Test Accuracy: 83.170006, LR: 0.100000, Current step value: 15620\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "Saving model with lowest val_loss = 0.5310\n",
      "\n",
      "\n",
      "Epoch 21, Loss: 0.5712, Accuracy: 81.7061, Test Loss: 0.5385, Test Accuracy: 82.660004, LR: 0.100000, Current step value: 16401\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 22, Loss: 0.5483, Accuracy: 82.4884, Test Loss: 0.6841, Test Accuracy: 78.619995, LR: 0.100000, Current step value: 17182\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 23, Loss: 0.5281, Accuracy: 82.9485, Test Loss: 0.5913, Test Accuracy: 82.599998, LR: 0.100000, Current step value: 17963\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 24, Loss: 0.5167, Accuracy: 83.2606, Test Loss: 0.5486, Test Accuracy: 82.910004, LR: 0.100000, Current step value: 18744\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 25, Loss: 0.5061, Accuracy: 83.8768, Test Loss: 0.5160, Test Accuracy: 84.220001, LR: 0.100000, Current step value: 19525\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "Saving model with lowest val_loss = 0.5160\n",
      "\n",
      "\n",
      "Epoch 26, Loss: 0.4882, Accuracy: 84.2210, Test Loss: 0.5001, Test Accuracy: 83.779999, LR: 0.100000, Current step value: 20306\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "Saving model with lowest val_loss = 0.5001\n",
      "\n",
      "\n",
      "Epoch 27, Loss: 0.4744, Accuracy: 84.8071, Test Loss: 0.5394, Test Accuracy: 84.580002, LR: 0.100000, Current step value: 21087\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 28, Loss: 0.4659, Accuracy: 85.0712, Test Loss: 0.5606, Test Accuracy: 84.270004, LR: 0.100000, Current step value: 21868\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 29, Loss: 0.4429, Accuracy: 85.9935, Test Loss: 0.4911, Test Accuracy: 84.250000, LR: 0.100000, Current step value: 22649\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "Saving model with lowest val_loss = 0.4911\n",
      "\n",
      "\n",
      "Epoch 30, Loss: 0.4278, Accuracy: 86.4397, Test Loss: 0.5370, Test Accuracy: 85.079994, LR: 0.100000, Current step value: 23430\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 31, Loss: 0.4287, Accuracy: 86.1956, Test Loss: 0.5950, Test Accuracy: 84.190002, LR: 0.100000, Current step value: 24211\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 32, Loss: 0.4258, Accuracy: 86.3296, Test Loss: 0.5239, Test Accuracy: 84.410004, LR: 0.100000, Current step value: 24992\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 33, Loss: 0.4141, Accuracy: 86.9338, Test Loss: 0.4889, Test Accuracy: 85.860001, LR: 0.100000, Current step value: 25773\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "Saving model with lowest val_loss = 0.4889\n",
      "\n",
      "\n",
      "Epoch 34, Loss: 0.4121, Accuracy: 87.1159, Test Loss: 0.5456, Test Accuracy: 83.700005, LR: 0.100000, Current step value: 26554\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 35, Loss: 0.4059, Accuracy: 87.0779, Test Loss: 0.4777, Test Accuracy: 85.869995, LR: 0.100000, Current step value: 27335\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "Saving model with lowest val_loss = 0.4777\n",
      "\n",
      "\n",
      "Epoch 36, Loss: 0.3885, Accuracy: 87.6861, Test Loss: 0.4883, Test Accuracy: 85.199997, LR: 0.100000, Current step value: 28116\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 37, Loss: 0.3861, Accuracy: 87.7241, Test Loss: 0.5061, Test Accuracy: 85.829994, LR: 0.100000, Current step value: 28897\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 38, Loss: 0.2674, Accuracy: 91.3492, Test Loss: 0.3836, Test Accuracy: 88.800003, LR: 0.010000, Current step value: 29678\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "Saving model with lowest val_loss = 0.3836\n",
      "\n",
      "\n",
      "Epoch 39, Loss: 0.2292, Accuracy: 92.4516, Test Loss: 0.3650, Test Accuracy: 89.060005, LR: 0.010000, Current step value: 30459\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "Saving model with lowest val_loss = 0.3650\n",
      "\n",
      "\n",
      "Epoch 40, Loss: 0.2120, Accuracy: 92.8937, Test Loss: 0.4011, Test Accuracy: 88.610001, LR: 0.010000, Current step value: 31240\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 41, Loss: 0.2077, Accuracy: 93.0098, Test Loss: 0.3846, Test Accuracy: 89.020004, LR: 0.010000, Current step value: 32021\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 42, Loss: 0.1955, Accuracy: 93.5659, Test Loss: 0.3704, Test Accuracy: 89.400002, LR: 0.010000, Current step value: 32802\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 43, Loss: 0.1886, Accuracy: 93.6400, Test Loss: 0.3601, Test Accuracy: 89.620003, LR: 0.010000, Current step value: 33583\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "Saving model with lowest val_loss = 0.3601\n",
      "\n",
      "\n",
      "Epoch 44, Loss: 0.1914, Accuracy: 93.5759, Test Loss: 0.3553, Test Accuracy: 89.630005, LR: 0.010000, Current step value: 34364\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "Saving model with lowest val_loss = 0.3553\n",
      "\n",
      "\n",
      "Epoch 45, Loss: 0.1836, Accuracy: 93.8100, Test Loss: 0.3659, Test Accuracy: 89.709999, LR: 0.010000, Current step value: 35145\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 46, Loss: 0.1733, Accuracy: 94.1701, Test Loss: 0.3777, Test Accuracy: 89.580002, LR: 0.010000, Current step value: 35926\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 47, Loss: 0.1716, Accuracy: 94.2422, Test Loss: 0.3568, Test Accuracy: 89.700005, LR: 0.010000, Current step value: 36707\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 48, Loss: 0.1695, Accuracy: 94.2602, Test Loss: 0.3705, Test Accuracy: 89.590004, LR: 0.010000, Current step value: 37488\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 49, Loss: 0.1641, Accuracy: 94.4242, Test Loss: 0.3746, Test Accuracy: 89.730003, LR: 0.010000, Current step value: 38269\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 50, Loss: 0.1611, Accuracy: 94.5903, Test Loss: 0.3730, Test Accuracy: 89.740005, LR: 0.010000, Current step value: 39050\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 51, Loss: 0.1593, Accuracy: 94.4882, Test Loss: 0.3795, Test Accuracy: 89.650002, LR: 0.010000, Current step value: 39831\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 52, Loss: 0.1540, Accuracy: 94.8644, Test Loss: 0.3977, Test Accuracy: 89.510002, LR: 0.010000, Current step value: 40612\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 53, Loss: 0.1517, Accuracy: 94.9084, Test Loss: 0.3689, Test Accuracy: 89.830002, LR: 0.010000, Current step value: 41393\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 54, Loss: 0.1486, Accuracy: 94.8924, Test Loss: 0.3910, Test Accuracy: 89.540001, LR: 0.010000, Current step value: 42174\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 55, Loss: 0.1428, Accuracy: 95.1705, Test Loss: 0.3758, Test Accuracy: 89.850006, LR: 0.001000, Current step value: 42955\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 56, Loss: 0.1399, Accuracy: 95.1845, Test Loss: 0.3769, Test Accuracy: 89.780006, LR: 0.001000, Current step value: 43736\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 57, Loss: 0.1390, Accuracy: 95.2565, Test Loss: 0.3774, Test Accuracy: 89.870003, LR: 0.001000, Current step value: 44517\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 58, Loss: 0.1380, Accuracy: 95.3065, Test Loss: 0.3733, Test Accuracy: 89.870003, LR: 0.001000, Current step value: 45298\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 59, Loss: 0.1397, Accuracy: 95.2725, Test Loss: 0.3739, Test Accuracy: 89.840004, LR: 0.001000, Current step value: 46079\n",
      "VGG-18: total # of trainable parameters = 20297674\n",
      "\n",
      "Epoch 60, Loss: 0.1405, Accuracy: 95.3205, Test Loss: 0.3721, Test Accuracy: 89.850006, LR: 0.001000, Current step value: 46860\n",
      "VGG-18: total # of trainable parameters = 20297674\n"
     ]
    }
   ],
   "source": [
    "# Train model using 'GradientTape'-\n",
    "    \n",
    "# Initialize parameters for Early Stopping manual implementation-\n",
    "# best_val_loss = 100\n",
    "# loc_patience = 0\n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "    '''\n",
    "    if loc_patience >= patience:\n",
    "        print(\"\\n'EarlyStopping' called!\\n\")\n",
    "        break\n",
    "    '''\n",
    "    \n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    \n",
    "    # variable to keep track of current training step-\n",
    "    curr_step = 0\n",
    "    \n",
    "    # Train using data augmentation (ImageDataGenerator())-\n",
    "    for x, y in datagen.flow(X_train, y_train, batch_size = batch_size, shuffle = True):\n",
    "    # for x, y in zip(augmented_train_batches, y_train):\n",
    "        train_one_step(orig_model, mask_model, optimizer, x, y)\n",
    "        # print(\"current step = \", curr_step)\n",
    "        curr_step += 1\n",
    "        \n",
    "        # break out of infinite loop to end current training epoch-\n",
    "        if curr_step >= X_train.shape[0] // batch_size:\n",
    "            # print(\"\\nTerminating training (datagen.flow())\")\n",
    "            break\n",
    "\n",
    "    # Testing dataset-\n",
    "    for x_t, y_t in test_dataset:\n",
    "    # for x_t, y_t in zip(validation_batches, y_test):\n",
    "        test_step(orig_model, optimizer, x_t, y_t)\n",
    "    \n",
    "    '''\n",
    "    for x, y in train_dataset:\n",
    "        train_one_step(winning_ticket_model, mask_model, optimizer, x, y)\n",
    "\n",
    "    for x_t, y_t in test_dataset:\n",
    "        test_step(winning_ticket_model, optimizer, x_t, y_t)\n",
    "\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    template = 'Epoch {0}, Loss: {1:.4f}, Accuracy: {2:.4f}, Test Loss: {3:.4f}, Test Accuracy: {4:4f}'\n",
    "    \n",
    "    print(template.format(\n",
    "        epoch + 1, train_loss.result(),\n",
    "        train_accuracy.result() * 100, test_loss.result(),\n",
    "        test_accuracy.result()  *100)\n",
    "         )\n",
    "    '''\n",
    "       \n",
    "    template = '\\nEpoch {0}, Loss: {1:.4f}, Accuracy: {2:.4f}, Test Loss: {3:.4f}, Test Accuracy: {4:4f}, LR: {5:.6f}, Current step value: {6}'\n",
    "\n",
    "    print(template.format(\n",
    "        epoch + 1, train_loss.result(), train_accuracy.result()*100,\n",
    "        test_loss.result(), test_accuracy.result()*100,\n",
    "        # optimizer.learning_rate.numpy()\n",
    "        optimizer._decayed_lr('float32').numpy(),\n",
    "        optimizer.iterations.numpy()\n",
    "    )\n",
    "         )\n",
    "    \n",
    "    history_lr[epoch + 1] = {'loss': train_loss.result(), 'acc': train_accuracy.result() * 100, 'val_loss': test_loss.result(),\n",
    "                                      'val_acc': test_accuracy.result() * 100, 'lr': optimizer._decayed_lr('float32').numpy(),\n",
    "                                      'step_val': optimizer.iterations.numpy()\n",
    "                                     }\n",
    "    \n",
    "    \n",
    "    # Count number of non-zero parameters in each layer and in total-\n",
    "    print(f\"VGG-18: total # of trainable parameters = {count_params(orig_model)}\")\n",
    "    \n",
    "    # Insert manual implementation of code for early stopping here.\n",
    "    \n",
    "    \n",
    "    # Save best weights achieved until now-\n",
    "    if (test_loss.result() < best_val_loss):    \n",
    "        # update 'best_val_loss' variable to lowest loss encountered so far-\n",
    "        best_val_loss = test_loss.result()\n",
    "        \n",
    "        print(f\"Saving model with lowest val_loss = {test_loss.result():.4f}\\n\")\n",
    "        \n",
    "        # Save trained model with validation accuracy-\n",
    "        orig_model.save_weights(\"VGG18_best_trained_loss.h5\", overwrite = True)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "165f9dfd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "3d2a08e5",
    "outputId": "a44774e1-c53b-42e2-8d5b-bfe55e614079"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished training VGG-18 CNN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFinished training VGG-18 CNN\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"VGG18_batchnorm_subclass_history_lr.pkl\", \"wb\") as file:\n",
    "    pickle.dump(history_lr, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38aa1336"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "406cd399"
   },
   "outputs": [],
   "source": [
    "# Initialize & load best weights model-\n",
    "best_model = VGG19()\n",
    "best_model.build(input_shape = (None, 32, 32, 3))\n",
    "# best_model = vgg18_nn()\n",
    "\n",
    "best_model.load_weights(\"VGG18_best_trained_loss.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model-\n",
    "best_model.compile(\n",
    "    loss=tf.keras.losses.categorical_crossentropy,\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate = warmup_shcedule, momentum = 0.9, decay = 0.0, nesterov = False),\n",
    "    # optimizer = tf.keras.optimizers.SGD(learning_rate = 0.01, momentum = 0.9),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "f391eb75"
   },
   "outputs": [],
   "source": [
    "# Evaluate trained model's validation loss and validation accuracy on validation dataset-\n",
    "loss, acc = best_model.evaluate(X_test, y_test, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "69ba58a5",
    "outputId": "b8edce97-a1f9-4e05-afe2-a74b631bf9c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trained VGG-18 CNN model metrics on validation dataset:\n",
      "validation loss = 27.2752 and validation accuracy = 89.6300%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTrained VGG-18 CNN model metrics on validation dataset:\")\n",
    "print(\"validation loss = {0:.4f} and validation accuracy = {1:.4f}%\\n\".format(loss, acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f378aed3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0d8a3596"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize learning rate during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lr = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in history_lr.keys():\n",
    "    plot_lr[epoch] = history_lr[epoch]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize learning rate during training-\n",
    "plt.figure(figsize = (9, 7))\n",
    "\n",
    "plt.plot(list(plot_lr.keys()), list(plot_lr.values()), label = 'learning rate')\n",
    "\n",
    "# plt.title(\"LeNet-300-100: LR & step values\")\n",
    "plt.title(\"LeNet-300-100: Warmup & Step-Decay (SGD) Learning Rate\")\n",
    "plt.xlabel(\"number of epochs\")\n",
    "plt.ylabel(\"LR\")\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "VGG18_Train_from_scratch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "research_paper",
   "language": "python",
   "name": "research_paper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
