{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import AveragePooling2D, Conv2D\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE:\n",
    "Please note that in the original paper \"Gradient-Based Learning Applied to Document Recognition\"\n",
    "by Yann LeCun et al, the original image size was of 32x32 pixels.\n",
    "However, in the TensorFlow MNIST dataset, the images are of 28x28 pixels.\n",
    "\n",
    "Also, in the original paper, hyperbolic tangent (tanh) activation functions were used in the hidden\n",
    "layers, however, in the code below, ReLU activation functions are used in the hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the MNIST dataset-\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and convert samples from integers to floating-point numbers-\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "X_train = tf.cast(X_train, dtype=tf.float32)\n",
    "X_test = tf.cast(X_test, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding for targets-\n",
    "n_classes = 10\n",
    "\n",
    "y_train_ohe = tf.keras.utils.to_categorical(y_train, n_classes)\n",
    "y_test_ohe = tf.keras.utils.to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.reshape(X_train, shape=(X_train.shape[0], 28, 28, 1))\n",
    "X_test = tf.reshape(X_test, shape = (X_test.shape[0], 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train.shape = (60000, 28, 28, 1) & y_train_ohe.shape = (60000, 10)\n",
      "X_test.shape = (10000, 28, 28, 1) & y_test_ohe.shape = (10000, 10)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nX_train.shape = {0} & y_train_ohe.shape = {1}\".format(X_train.shape, y_train_ohe.shape))\n",
    "print(\"X_test.shape = {0} & y_test_ohe.shape = {1}\\n\".format(X_test.shape, y_test_ohe.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training LeNet-5 CNN for MNIST classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LeNet-5 CNN model for MNIST classification:\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional layer-1\n",
    "model.add(Conv2D(filters = 6, kernel_size = (5, 5),\n",
    "    strides = (1, 1), activation = 'relu', input_shape = (28, 28, 1)))\n",
    "\n",
    "# Average pooling layer-1\n",
    "model.add(AveragePooling2D(pool_size = (2, 2), strides = (2, 2)))\n",
    "\n",
    "# Convolutional layer-2\n",
    "model.add(Conv2D(filters = 16, kernel_size = (5, 5),\n",
    "    strides = (1, 1), activation = 'relu'))\n",
    "\n",
    "# Average pooling layer-2\n",
    "model.add(AveragePooling2D(pool_size = (2, 2), strides = (2, 2)))\n",
    "\n",
    "# Flatten the output-\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Fully connected layer-\n",
    "model.add(layers.Dense(units = 120, activation = 'relu'))\n",
    "\n",
    "# Another flattening of the previous layer-\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Fully connected layer-\n",
    "model.add(layers.Dense(units = 84, activation = 'relu'))\n",
    "\n",
    "# Output layer-\n",
    "model.add(layers.Dense(units = 10, activation = 'softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the designed nn model-\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "        optimizer=tf.keras.optimizers.SGD(lr = 0.1,\n",
    "            momentum = 0.0, decay = 0.0),\n",
    "        metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save INITIAL WEIGHTS before training model-\n",
    "model.save_weights(\"LeNet-5_MNIST_INITIAL_WEIGHTS.h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping-\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 5, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 0.5166 - accuracy: 0.8381 - val_loss: 0.1588 - val_accuracy: 0.9536\n",
      "Epoch 2/30\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.1348 - accuracy: 0.9580 - val_loss: 0.1028 - val_accuracy: 0.9692\n",
      "Epoch 3/30\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.0934 - accuracy: 0.9711 - val_loss: 0.0799 - val_accuracy: 0.9750\n",
      "Epoch 4/30\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.0728 - accuracy: 0.9773 - val_loss: 0.0658 - val_accuracy: 0.9794\n",
      "Epoch 5/30\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.0608 - accuracy: 0.9804 - val_loss: 0.0535 - val_accuracy: 0.9823\n",
      "Epoch 6/30\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.0514 - accuracy: 0.9837 - val_loss: 0.0547 - val_accuracy: 0.9828\n",
      "Epoch 7/30\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.0449 - accuracy: 0.9858 - val_loss: 0.0523 - val_accuracy: 0.9840\n",
      "Epoch 8/30\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.0399 - accuracy: 0.9875 - val_loss: 0.0495 - val_accuracy: 0.9843\n",
      "Epoch 9/30\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.0364 - accuracy: 0.9885 - val_loss: 0.0525 - val_accuracy: 0.9839\n",
      "Epoch 10/30\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.0324 - accuracy: 0.9895 - val_loss: 0.0493 - val_accuracy: 0.9850\n",
      "Epoch 11/30\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.0290 - accuracy: 0.9907 - val_loss: 0.0508 - val_accuracy: 0.9855\n",
      "Epoch 12/30\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.0267 - accuracy: 0.9913 - val_loss: 0.0422 - val_accuracy: 0.9874\n",
      "Epoch 13/30\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.0233 - accuracy: 0.9924 - val_loss: 0.0542 - val_accuracy: 0.9838\n",
      "Epoch 14/30\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.0222 - accuracy: 0.9929 - val_loss: 0.0590 - val_accuracy: 0.9824\n",
      "Epoch 15/30\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.0200 - accuracy: 0.9933 - val_loss: 0.0453 - val_accuracy: 0.9863\n",
      "Epoch 16/30\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.0188 - accuracy: 0.9939 - val_loss: 0.0556 - val_accuracy: 0.9836\n",
      "Epoch 17/30\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.0162 - accuracy: 0.9948 - val_loss: 0.0418 - val_accuracy: 0.9885\n",
      "Epoch 18/30\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.0143 - accuracy: 0.9957 - val_loss: 0.0434 - val_accuracy: 0.9869\n",
      "Epoch 19/30\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.0131 - accuracy: 0.9957 - val_loss: 0.0412 - val_accuracy: 0.9881\n",
      "Epoch 20/30\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.0120 - accuracy: 0.9964 - val_loss: 0.0430 - val_accuracy: 0.9884\n",
      "Epoch 21/30\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.0110 - accuracy: 0.9969 - val_loss: 0.0444 - val_accuracy: 0.9881\n",
      "Epoch 22/30\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.0103 - accuracy: 0.9966 - val_loss: 0.0484 - val_accuracy: 0.9869\n",
      "Epoch 23/30\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.0095 - accuracy: 0.9972 - val_loss: 0.0479 - val_accuracy: 0.9873\n",
      "Epoch 24/30\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.0082 - accuracy: 0.9977 - val_loss: 0.0493 - val_accuracy: 0.9875\n"
     ]
    }
   ],
   "source": [
    "# Train model-\n",
    "history = model.fit(x=X_train, y=y_train_ohe,\n",
    "\tepochs=30, batch_size=128,\n",
    "\tvalidation_data=(X_test, y_test_ohe),\n",
    "\tcallbacks = [callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluating trained model-\n",
    "plt.imshow(tf.reshape(X_test[234], shape = (28, 28)), cmap = \"Greys\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Make prediction using trained model-\n",
    "prediction = model.predict(tf.reshape(X_test[234], shape = (1, 28, 28, 1)))\n",
    "# number of examples, 28, 28, number of channels\n",
    "\n",
    "prediction.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04116819323307245, 0.9881]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evalutating trained model-\n",
    "model.evaluate(X_test, y_test_ohe, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save FINAL WEIGHTS after training model-\n",
    "model.save_weights(\"LeNet-5_MNIST_FINAL_WEIGHTS.h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
